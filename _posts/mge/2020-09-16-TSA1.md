---
title: "[Time Series Analysis 0] Intro"
tags: Time-Series-Analysis Management-Engineering
toc: true
---

# Intro
**시계열 분석(time series analysis)는 시간에 따라 기록 및 관찰된 데이터와 통계를 분석하는 분야다.** 주가, 인구, 기온 등 많은 데이터가 그것의 시간적 흐름과 그에 따른 변화를 포함하고 있다. 시계열 분석에서는 이들을 분석하고 예측하기 위한 적절한 모델을 고안해내는 것이 목표다.

하지만 이는 꽤 어려운 일인데, 기본적으로 conventional한 통계적 방법이 일부 제한되기 때문이다. 가령, 일반적으로 우린 많은 관측들이 IID하다고 가정하지만, 시계열에서는 그렇게 가정하기가 어렵다. 인접한 관측 시점에서의 데이터와 많은 상관관계를 가지기 때문이다. "어제 기온이 25도였을 때 오늘이 26도일 확률과, 어제 기온이 영하 10도였을 때 오늘이 26도일 확률이 같을까?" 같은 느낌이다.


# Fundamental Concepts
본격적으로 시계열 분석에 대해 배우기 전, 몇 가지 기본 개념을 정리하고 가자.

## Time Series / Stochastic Process
시계열(time series) 데이터란, 여러 시점에서 관찰된 데이터들의 series를 의미한다. 관찰 시점은 equally spaced한 게 일반적이며, 이들은 주로 line-chart로 표현된다. 우리의 목표는 그렇게 주어진 샘플 데이터를 설명하기 위한 적절한 수학적 모델(mathematical model)을 찾는 것이다. 이를 위해서, 우리는 시계열을 확률 과정(stochastic process)로 바라볼 수 있다. 즉, 데이터를 잘 설명하는 적절한 확률 과정을 찾는 문제가 된다.

## Statistics
확률 과정에서의 몇몇 통계량을 되짚어보자. 확률 과정 $\{Y_t: t = 0, \pm 1, \pm 2, \cdots \}$에 대하여...

- Mean function: $\mu_t = E(Y_t)$
- Autocovariance function: $\gamma_{t, s} = Cov(Y_t, Y_s)$ where $Cov(Y_t, Y_s) = E[(Y_t - \mu_t)(Y_s - \mu_s)] =$ $E(Y_tY_s) - \mu_t\mu_s$
- Autocorrelation function(ACF): $\rho_{t, s} = Corr(Y_t, Y_s)$ where $Corr(Y_t, Y_s) =$ $\frac{Cov(Y_t, Y_s)}{\sqrt{Var(Y_t)Var(Y_s)}} = $ $\frac{\gamma_{t, s}}{\sqrt{\gamma_{t,t}\gamma_{s,s}}}$

Covariance와 Correlation은 두 대상 사이의 선형 독립(linear dependency)의 지표가 된다. 특히, Correlation은 unitless하기 때문에 독립성을 판단하기 더 용이하다.

더 자세한 건 아래의 Random Walk Example을 통해 알아보자.

### Random Walk
랜덤 워크는 확률 과정을 이야기할 때 자주 언급되는 예시다. 다음과 같은 상황을 생각해보자.

> 한 사람이 수직선 위에 서 있다. 매 걸음 전에, 동전을 던져 앞면이 나오면 앞으로 1m를, 뒷면이 나오면 뒤로 1m를 걷는다. $Y_t$는 $t$번째 동전을 던진 후 사람이 서 있는 위치고, $e_n$은 $n$번째 동전을 던지는 이벤트에 대한 확률 변수다.

이 문제는 여러 바리에이션이 가능하다. 꼭 절반 확률인 동전일 필요도 없고, 움직이는 방향도 n-차원으로 확장될 수 있을 것이다. 지금은 각각의 이벤트가 IID하며 zero mean이고 분산이 ${\sigma_e}^2$인 경우를 생각해보자.

시계열 ${Y_t: t = 1, 2, 3, \cdots}$은 다음과 같이 나타낼 수 있다.

$Y_1 = e_1$
<br>$Y_2 = e_1 + e_2$
<br>$\cdots$
<br>$Y_t = e_1 + e_2 + \cdots + e_t$

대신, 이렇게 표현할 수도 있다.

$Y_t = Y_{t-1} + e_t$ with initial condition $Y_1 = e_1$

먼저, 평균을 계산해보자.

$\mu_t = E(Y_t) = E(e_1 + \cdots + e_t)$
<br>$= E(e_1) + \cdots + E(e_t)$ (by linearity of mean)
<br>$= 0+0+\cdots + 0$
<br>$=0$

분산은?

$Var(Y_t) = Var(e_1 + e_2 + \cdots + e_t)$
<br>$=Var(e_1) + Var(e_2) + \cdots + Var(e_t)$
<br>$={\sigma_e}^2 +{\sigma_e}^2 + \cdots + {\sigma_e}^2$
<br>$=t{\sigma_e}^2$

이제 autocovariance를 구해보자. 그나저나 한국어론 뭐라 표현하면 좋을까, 자기공분산? 아무튼..

$\gamma_{t, s} = Cov(Y_t, Y_s)$
<br> $=Cov(e_1 + e_2 + \cdots + e_t, e_1 + e_2 + \cdots + e_s)$
<br> $\sum_{i=1}^{s} \sum_{j=1}^{t} Cov(e_i, e_j) $ (Why? (1))
<br> $t{\sigma_e}^2$ (Why? (2))

(1)은 공분산의 아래와 같은 성질 때문이다. 합을 밖으로 빼는 방법인데,

$Cov[\sum(c_iY_{t_i}), \sum(d_jY_{s_j})] = \sum_i \sum_j c_id_j Cov(Y_{t_i}, Y_{s_j}$

(2)도 공분산의 정의에 근거한다. $e_i$와 $e_j$는 (서로 같지 않은 이상) IID이고, 자기 자신과의 공분산은 분산과 같으니 다음과 같이 표기할 수 있다.

$ Cov(e_i, e_j) = 
\begin{cases}
    {\sigma_e}^2 & (i = j) \\
    0 & (\text{otherwise}) 
\end{cases}$ 




