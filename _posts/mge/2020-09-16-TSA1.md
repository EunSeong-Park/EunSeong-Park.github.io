---
title: "[Time Series Analysis 0] Intro"
tags: Time-Series-Analysis Management-Engineering
toc: true
---

# Intro
**시계열 분석(time series analysis)는 시간에 따라 기록 및 관찰된 데이터와 통계를 분석하는 분야다.** 주가, 인구, 기온 등 많은 데이터가 그것의 시간적 흐름과 그에 따른 변화를 포함하고 있다. 시계열 분석에서는 이들을 분석하고 예측하기 위한 적절한 모델을 고안해내는 것이 목표다.

하지만 이는 꽤 어려운 일인데, 기본적으로 conventional한 통계적 방법이 일부 제한되기 때문이다. 가령, 일반적으로 우린 많은 관측들이 IID하다고 가정하지만, 시계열에서는 그렇게 가정하기가 어렵다. 인접한 관측 시점에서의 데이터와 많은 상관관계를 가지기 때문이다. "어제 기온이 25도였을 때 오늘이 26도일 확률과, 어제 기온이 영하 10도였을 때 오늘이 26도일 확률이 같을까?" 같은 느낌이다.


# Fundamental Concepts
본격적으로 시계열 분석에 대해 배우기 전, 몇 가지 기본 개념을 정리하고 가자.

## Time Series / Stochastic Process
시계열(time series) 데이터란, 여러 시점에서 관찰된 데이터들의 series를 의미한다. 관찰 시점은 equally spaced한 게 일반적이며, 이들은 주로 line-chart로 표현된다. 우리의 목표는 그렇게 주어진 샘플 데이터를 설명하기 위한 적절한 수학적 모델(mathematical model)을 찾는 것이다. 이를 위해서, 우리는 시계열을 확률 과정(stochastic process)로 바라볼 수 있다. 즉, 데이터를 잘 설명하는 적절한 확률 과정을 찾는 문제가 된다.

## Statistics
확률 과정에서의 몇몇 통계량을 되짚어보자. 확률 과정 $\{Y_t: t = 0, \pm 1, \pm 2, \cdots \}$에 대하여...

- Mean function: $\mu_t = E(Y_t)$
- Autocovariance function: $\gamma_{t, s} = Cov(Y_t, Y_s)$ where $Cov(Y_t, Y_s) = E[(Y_t - \mu_t)(Y_s - \mu_s)] =$ $E(Y_tY_s) - \mu_t\mu_s$
- Autocorrelation function(ACF): $\rho_{t, s} = Corr(Y_t, Y_s)$ where $Corr(Y_t, Y_s) =$ $\frac{Cov(Y_t, Y_s)}{\sqrt{Var(Y_t)Var(Y_s)}} = $ $\frac{\gamma_{t, s}}{\sqrt{\gamma_{t,t}\gamma_{s,s}}}$

Covariance와 Correlation은 두 대상 사이의 선형 독립(linear dependency)의 지표가 된다. 특히, Correlation은 unitless하기 때문에 독립성을 판단하기 더 용이하다.

더 자세한 건 아래의 Random Walk Example을 통해 알아보자.

### Random Walk
랜덤 워크는 확률 과정을 이야기할 때 자주 언급되는 예시다. 다음과 같은 상황을 생각해보자.

> 한 사람이 수직선 위에 서 있다. 매 걸음 전에, 동전을 던져 앞면이 나오면 앞으로 1m를, 뒷면이 나오면 뒤로 1m를 걷는다. $Y_t$는 $t$번째 동전을 던진 후 사람이 서 있는 위치고, $e_n$은 $n$번째 동전을 던지는 이벤트에 대한 확률 변수다.

이 문제는 여러 바리에이션이 가능하다. 꼭 절반 확률인 동전일 필요도 없고, 움직이는 방향도 n-차원으로 확장될 수 있을 것이다. 지금은 각각의 이벤트가 IID하며 zero mean이고 분산이 ${\sigma_e}^2$인 경우를 생각해보자.

시계열 ${Y_t: t = 1, 2, 3, \cdots}$은 다음과 같이 나타낼 수 있다.

$Y_1 = e_1$
<br>$Y_2 = e_1 + e_2$
<br>$\cdots$
<br>$Y_t = e_1 + e_2 + \cdots + e_t$

대신, 이렇게 표현할 수도 있다.

$Y_t = Y_{t-1} + e_t$ with initial condition $Y_1 = e_1$

먼저, 평균을 계산해보자.

$\mu_t = E(Y_t) = E(e_1 + \cdots + e_t)$
<br>$= E(e_1) + \cdots + E(e_t)$ (by linearity of mean)
<br>$= 0+0+\cdots + 0$
<br>$=0$

분산은?

$Var(Y_t) = Var(e_1 + e_2 + \cdots + e_t)$
<br>$=Var(e_1) + Var(e_2) + \cdots + Var(e_t)$
<br>$={\sigma_e}^2 +{\sigma_e}^2 + \cdots + {\sigma_e}^2$
<br>$=t{\sigma_e}^2$

이제 autocovariance를 구해보자. 그나저나 한국어론 뭐라 표현하면 좋을까, 자기공분산? 아무튼..

$\gamma_{t, s} = Cov(Y_t, Y_s)$
<br> $=Cov(e_1 + e_2 + \cdots + e_t, e_1 + e_2 + \cdots + e_s)$
<br> $=\sum_{i=1}^{s} \sum_{j=1}^{t} Cov(e_i, e_j) $ (Why? (1))
<br> $=t{\sigma_e}^2$ (Why? (2))

(1)은 공분산의 아래와 같은 성질 때문이다. 합을 밖으로 빼는 방법인데,

$Cov[\sum(c_iY_{t_i}), \sum(d_jY_{s_j})] = \sum_i \sum_j c_id_j Cov(Y_{t_i}, Y_{s_j})$

(2)도 공분산의 정의에 근거한다. $e_i$와 $e_j$는 (서로 같지 않은 이상) IID이어서 0이고, 자기 자신과의 공분산은 분산과 같으니 위와 같은 결과가 나온다.

마지막으로 autocorrelation을 구해보자.

$\rho_{t, s} = \frac{\gamma_{t,s}}{\sqrt{\gamma_{t,t}\gamma_{s,s}}} =$ $\frac{t{\sigma_e}^2}{\sqrt{ts{\sigma_e}^4}} = \sqrt{\frac{t}{s}}$

와 정말 간단한걸! 여기에서 더 나아가, 이 결과가 의미하는 바를 생각해보자. 예를 들어, 두 시점이 가까울 수록 더욱 연관되어있고, 인접한 두 시점은 시간이 흐를 수록 더욱 연관 되있다고 볼 수 있다. ($\rho_{1,2}$와 $\rho_{100,101}$을 비교해보자.)


## Stationarity
많은 분야에서, 많은 문제에 대해 그래왔듯이, 적당한 가정은 문제의 단순화에 도움이 된다. 가령 물리 문제에서도 마찰이 없는 평면에서 물건을 밀고 당겼듯이. 확률 과정에서도 그러한 가정이 몇 있는데, 대표적으로 할 수 있는 가정이 정상성(stationarity)이다. 그런데 정상성이 뭐냐?

정상성이란 시간의 흐름에 따라 확률/통계적인 성질이 일관적으로 유지되는 성질을 말한다. 아래의 예시들을 살펴보자.

![](/imgs/mge/tsa1.png)

이러한 데이터는 아마 분산은 일정한데 평균이 일정하지 않은 케이스일 것이다. 데이터의 trend가 명확하게 드러난다.

![](/imgs/mge/tsa2.png)

위와 같은 데이터는 평균은 0으로 일정하나, 분산은 점점 커지는 형태다.

정상성을 가진 데이터는, 시간의 흐름에 따라 평균과 분산 등에 명확한 변화가 없다. 이러한 성질은 많은 tool을 사용할 수 있도록 돕기 때문에, 시계열 분석을 위한 열쇠가 된다.

### Strict(Strong) Stationarity
강한 정상성은, 어떤 시점의 임의의 joint PDF가 어떤 time tag, $k$만큼 이동해도 변하지 않음을 의미한다. 즉,

$f(Y_{t_1}, Y_{t_2}, \cdots, Y_{t_n}) =$ $f(Y_{t_1+k}, Y_{t_2+k}, \cdots, Y_{t_n+k})$

그러면 이런 의문이 들게 된다. "그럼 모든 joint와 time lag에 대해 불변함을 보여야 하나? 어떻게?" $n=1$인 경우 (univariate)나 $n=2$인 경우는 어떻게든 보일 수 있을 것 같다. 하지만 모든 $n$에 대해서는 너무 어렵지 않을까?

그래서 나온 게 약한(weak) 정상성이다. 조금 더 느슨한 조건으로 정상성을 이끌어내어 보자.

### Weak Stationarity
약한 정상성은 다음과 같은 성질을 가짐을 의미한다.

- 평균, 분산이 상수함수다.
- 자기공분산(autocovariance)는 두 시점 간의 시차에만 의존하고, 각 시점에는 의존하지 않는다. 즉, $\gamma_{t, t-k} = \gamma_{0, k}$ for all $t, k$

이는 joint한 시점의 개수 $n$이 1, 2일 때의 강한 정상성과 같다.


### White Noise
화이트 노이즈 프로세스는 대표적인 정상 과정(stationary process)의 예시다. 시점 $t$에 따른 IID R.V. $e_t$를 생각하자. 이는 zero mean을 가지고, 분산은 시점에 상관없이 ${\sigma_e}^2$다. 이러한 시퀀스, 시계열을 생각하고, 정상성을 보여보자.

화이트 노이즈는 강한 정상성도 보이기 쉽다.

$P(e_{t_1} \le x_1, e_{t_2} \le x_2, \cdots, e_{t_n} \le x_n)$
<br>$= $P(e_{t_1} \le x_1)P(e_{t_2} \le x_2) \cdots P(e_{t_n} \le x_n)$ (by independence)
<br>$= P(e_{t_1 -k} \le x_1)P(e_{t_2 -k} \le x_2) \cdots P(e_{t_n -k} \le x_n)$ (by identical distribution)
<br>$=P(e_{t_1-k}\le x_1, e_{t_2-k}\le x_2, \cdots, e_{t_n-k}\le x_n$ (by independence)

와! 

화이트 노이즈는 많은 확률 과정을 세우는 기반이 된다는 점에서 유용하다. 하지만 이건 나중에 알아볼 생각이고, 여기서 Intro는 끝!


