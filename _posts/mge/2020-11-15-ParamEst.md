---
title: "[Time Series Analysis 5] Parameter Estimation"
tags: Time-Series-Analysis Management-Engineering Data-Science
toc: true
---

# Intro
지난 포스트에선 주어진 시계열 데이터를 어떻게 모델링할지에 대해 알아보았다. 모델을 결정했다면, 이제 그 모델에 대한 **parameter**를 결정해야 한다. 어떻게 적당한 파라미터를 추정할 수 있을까?


# Parameter Estimation
가령 우리가 관측한 시계열에 대해 **ARIMA**($p,d,q$)로 모델을 결정했다면, $\phi_1, \cdots, \phi_p$와 $ \theta_1, \cdots \theta_q$의 값을 추정해야 한다. 여기서, 우리는 그 대신 $d^{th}$ difference(즉, **ARMA($p,q$)**)의 파라미터를 추정할 수도 있다.

이러한 맥락에서, 우리는 여기서 프로세스들이 stationary하다고 생각할 것이다.

## Method of Moments
**Method of moments** 은 파라미터 추정을 위한, 쉽고 자주 사용되는 방법 중 하나다. 주로 다음과 같은 절차에 의해 이루어진다.

- Sample moments와 그에 대응되는 theoretical moments를 equating한다.
- Resulting equation을 풀어 파라미터의 추정치를 얻는다.
  
예시를 살펴보자.

### AR(1) Models

$$Y_t= \phi Y_{t-1} + e_t$$

우리는 $\rho_k = \phi^k$라는 사실을 알고 있다. 즉, lag $1$ sample ACF, $r_1$을 이용하여,

$$\hat \phi = \hat \rho_1 = r_1$$

### AR(2) Models

$$Y_t= \phi_1 Y_{t-1} + \phi_2Y_{t-2} + e_t$$

Yule-Walker equation에 의하여,

$$\rho_1 = \phi_1 + \rho_1\phi_2, \;\; \rho_2 = \rho_1 \phi_1 + \phi_2$$

즉,

$$\hat \phi_1 = \frac{r_1(1-r_2)}{1-r_1^2},\;\; \hat \phi_2 = \frac{r_2 - r_1^2}{1-r_1^2}$$

### MA(1) Models

$$Y_t = e_t - \theta e_{t-1}$$

$\rho_1 = - {\theta\over 1+ \theta^2}$이므로, $\rho_1$과 $r_1$을 equating하고, quadratic equation을 풂으로써, $\theta$를 추정할 수 있다.

## Least Squares Estimation
**Least Squares Estimation**은 어디서나 항상 나오는 **LSM(least squares method)**를 이용한 추정 방법이다. 즉, 잔차(residual)의 제곱을 최소화하는 방향으로 추정한다.

### AR(1) Models
어떤 nonzero mean **AR(1)** 프로세스를 생각해보자.

$$Y_t - \mu = \phi(Y_{t-1} - \mu) + e_t$$

우리는 이걸 predictor variable, $Y_{t-1}$과 response variable, $Y_t$에 대한 회귀 모델로 볼 수도 있다. 그러면 least square estimation은 다음과 같은 difference의 제곱을 최소화하는 방향으로 이루어질 수 있다.

$$(Y_t - \mu) - \phi(Y_{t-1} - \mu) $$

즉, 

$$S_c(\phi, \mu) = \sum_{t=2}^n ((Y_t - \mu) - \phi(Y_{t-1} - \mu))^2$$

이것을 보통 **conditional sum-of-squares function**이라고 부른다.

### MA(1) Models

$$ Y_t = e_t - \theta e_{t-1}$$

Least squares method를 적용하기 위해, 가역적인 **MA(1)** 모델을 다음과 같이 변형하자.

$$Y_t = -\theta Y_{t-1} - \theta^2 Y_{t-2} - \cdots +e_t$$

그러면 다음과 같이 least squares를 사용할 수 있다.

$$S_c(\theta) = \sum e_t^2 = \sum(Y_t + +\theta Y_{t-1} + \theta^2 Y_{t-2} + \cdots)^2$$

## Maximum Likelihood Estimation
Maximum likelihood estimation에 대해 알아보기 전에, **likelihood**에 대해 알아볼 필요가 있다.

### Likelihood
코인 토스를 예시로 생각해보자. 당연히 우리는 앞면이 나올 **확률**이 $1 \over 2$임을 알고 있다. 그러면 코인이 $5$번 연속으로 앞면일 확률은 $1 \over 32$이다. 그럼 반대로 생각해보자. 앞면이 연속으로 $5$번 나온 걸 확인했을 때, (당연히 잘 만들어진 동전이라면 $1 \over 2$겠지만) **우리는 코인이 앞면을 보일 확률을 어떻게 예측할까?** 이럴 때 사용하는 개념이 **likelihood**다. 

**likelihood**는 **특정 observation이 주어진 상태에서, 특정 파라미터가 출현할 가능성**이다. 확률(probability)는 특정 파라미터가 주어진 상태에서 특정 observation이 출현할 가능성임을 생각하면, 미묘하지만 분명히 서로 다르다. Likelihood의 개념을 생각해보면, 우리의 model specification 과정과 딱 들어맞는 것 같다.

### Maximum Likelihood Method
그래서, 우리는 likelihood가 가장 높은 파라미터를 찾으려 하고, 그것을 위한 방법이 **maximum likelihood method**다. 

우리는 likelihood function을 정의하고, 그것이 최댓값을 가지도록 하는 파라미터를 찾을 것이다.

### AR(1) Models
어떤 nonzero mean **AR(1)** 프로세스를 생각해보자.

$$Y_t - \mu = \phi(Y_{t-1} - \mu) + e_t$$

우리는 여기서 화이트 노이즈가 독립적이고 $\mathcal N(0, \sigma_e^2)$ ($\sigma_e$는 common stddev)를 따른다고 가정한다.

각각 $e_t$의 PDF는 다음과 같다.

$$\frac{1}{\sqrt{2\pi \sigma_e^2}}e^{-\frac{e_t^2}{2\sigma_e^2}}$$

또, 각각은 독립적이므로, $e_2, e_3, \cdots, e_n$의 joint PDF는...

$$\frac{1}{\sqrt{2\pi \sigma_e^2}^{(n-1)}}e^{-\frac{1}{2\sigma_e^2}\sum_{t=2}^n e_t^2}$$

$Y_n-\mu = \phi(Y_{n-1}-\mu)+e_n$임을 기억하자. 그러면 $Y_1 = y_1$을 조건부로 한 joint PDF는 다음과 같다,

$$(2\pi\sigma_e^2)^{\frac{n-1}{2}}\text{exp}(-\frac{1}{2\sigma_e^2}\sum_{t=2}^n((y_t-\mu)-\phi(y_{t-1}-\mu))^2$$

**AR(1)**의 linear process representation으로부터,

$Y_1 \sim \mathcal N(\mu, \frac{\sigma_e^2}{1-\phi^2})$

이제 아까 구한 PDF에 $Y_1$의 marginal PDF($P(Y_1)$)를 곱해주면, $Y_1, Y_2, \cdots, Y_n$에 대한 joint PDF가 나온다. 이를 적당히 정리해주면, 다음과 같은 likelihood function을 구할 수 있다.

$$L(\phi, \mu, \sigma_e^2) = (2\pi\sigma_e^2)^{-\frac{n}{2}}(1-\phi^2)^\frac{1}{2}\text{exp}(-\frac{1}{2\sigma_e^2})S(\phi, \mu)) \\ \text{where } S(\phi, \mu) = \sum_{t=2}^n ((Y_t-\mu) - \phi(Y_{t-1}-\mu))^2 + (1-\phi^2)(Y_1 -\mu)$$

참고로 위의 $S(\phi,\mu)$는 **unconditional sum-of-squares function**이라고 한다.
