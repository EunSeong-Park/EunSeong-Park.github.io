---
title: "메모리 계층 구조 - 캐시"
tags: Computer-Architecture
toc: true
---

# Basic Principle
많은 CS 과목에서 **메모리 계층 구조(memory hierarchy)**에 대한 개념을 한 번은 꼭 짚고 넘어간다. 지금도 한 세 번째는 우려먹는 것 같다. 아무튼 그래서 전반적인 내용은 알고 있다고 가정하고 정리하려 한다.

컴퓨터 내엔 여러 종류의 메모리가 있고, 이들은 용량, 접근 속도, 용도 등의 측면에서 서로 다르다. 예를 들어 SRAM은 빠르지만 용량 대비 굉장히 비싸고, 디스크는 가격 대비 매우 큰 용량을 지니고 있지만 무척 느리다. 또, 빠른 메모리일 수록 CPU와 (개념적으로) 가까이 있다.

![](/imgs/ca/ca2.png)

즉, 속도와 비용은 동시에 만족하기가 굉장히 어렵다. 이러한 한계를 극복하기 위한 시도 중 하나가 메모리 계층 구조를 활용하는 것이다.


# Locality
메모리 계층 구조를 논할 때, **지역성(locality)**은 항상 빠지지 않는다. 왜냐? 메모리 계층 구조는 좋은 지역성을 가진 프로그램을 가정하고, 그에 맞추어 성능적인 측면에서의 이점을 가져가기 때문이다. 달리 말하면, 나쁜 지역성을 가진 프로그램의 경우, 좋은 지역성의 프로그램보다 나쁜 퍼포먼스를 보일 것이다.

즉, 우리는 메모리 계층 구조를 설계하고 이해할 때, 그것이 지역성을 잘 활용하여 좋은 성능을 이끌어내는지 확인해야 한다. 프로그램을 설계할 때도, (우리가 사용하는 컴퓨터는 지역성을 충분히 활용함을 알고 있으므로) 그것이 좋은 지역성을 가지는지 확인해야 한다.

지역성은 두 종류로 나눌 수 있다.

- **시간적 지역성(temporal locality)**: 최근에 액세스한 대상은 이후에 또 다시 액세스할 가능성이 크다.
- **공간적 지역성(spatial locality)**: 최근에 액세스한 대상 근처의 대상도 액세스될 가능성이 크다.

시간적 지역성의 대표적인 예시는 for loop의 `i` 등이 있고, 공간적 지역성의 대표적인 예시는 순차적인 명령어 접근이나 배열(array) 등이 있을 것이다.

```c
for (int i=0; i < n; i++){
	for (int j=0; j < n; j++){
		Arr[j][i] = 0;
	}	
}
```

위와 같은 코드는 공간적 지역성이 잘 지켜지지 않은 케이스다. 배열은 row-major하게 할당되는데, 저러한 방식은 주소를 계속 건너뛰면서 참조하기 때문이다. 


# Cache Memory
보통 캐시(cache)는 상대적인 개념으로, 낮은 계층의 데이터를 가져와 접근 속도를 높이는 방법 혹은 그러한 역할을 수행하는 계층을 의미한다. 캐시 메모리(cache memory)는 CPU와 가장 가까이 있어, 메인 메모리로부터 캐시 역할을 하는 메모리를 의미한다.

그 전에, 용어를 간단히 정리하고 가자.

## Terminologies
블록(block), 또는 라인(line)은 캐시에서의 복사본의 기본 단위다. 각각의 블록이 가지는 크기는 아키텍처마다 다를 수 있다. 하나의 워드만 포함할 수도, 몇 개의 워드를 포함할 수도 있다.

![](/imgs/ca/ca1.png)

우리는 낮은 계층의 메모리가 가진 데이터에 접근하려 할 때, 직접 그곳에 접근하기보단 캐시를 활용한다. 하지만 모든 데이터가 캐시 내에 있지는 않다. 찾던 데이터가 캐시에 있는 상황을 **hit**라 하고, 그렇지 않은 상황을 **miss**라 한다.

캐시 미스가 발생했을 때, 캐시는 낮은 계층으로부터 해당 데이터를 가져와야 한다. 이 때 걸리는 시간을 **miss panelty**라 하며, 캐시 미스가 발생하는 비율을 **miss rate**라 한다.

## Mapping
캐시는 하위 계층으로부터 데이터를 가져와 어떻게 저장할까? 또, 해당 데이터를 어떻게 찾을 수 있을까? 크게 세 방법으로 나눌 수 있다.

- **Direct-mapped**: 하위 계층의 주소를 이용해 캐시 내 주소를 유일하게 결정한다. (1-way associative)
- **n-way associative**: 캐시를 n개의 블록을 가지는 집합으로 분할하여, 하위 계층의 주소를 이용해 캐시 내 집합을 유일하게 결정한다.
- **fully associative**: 블록은 캐시 내 어디든지 위치할 수 있다.

각각은 장단점을 지니고 있다. 직접 매핑(direct-mapped) 캐시는 블록을 찾는 데 비용이 거의 들지 않지만, fully associative하다면 비용이 상당히 발생할 것이다. 반대로, 직접 매핑 캐시에서 같은 위치에 캐시되는 두 블록이 번갈아가며 참조되는 경우를 생각해볼 수 있을 것이다. 블록을 교체하는 과정이 계속 반복되는 셈이다. 연관된(associative) 캐시는 그러한 경쟁이 적기 때문에 이러한 점에선 효과적이다.

각각의 방법을 하나하나 알아볼 예정이다. 또, 직접 매핑 방식에서 캐시에 대한 전반적인 동작 과정을 알아보도록 하자.

## Direct-Mapped Cache & Cache Basics
캐시가 어떻게 작동하는지 알아보자. 이를 위해 우선 직접 매핑 캐시를 먼저 알아보고, 이를 기반으로 캐시의 동작 과정을 살펴볼 것이다.

직접 매핑 캐시에서는, 주어진 메모리 블록 주소에 의해 캐시 내 위치가 유일하게 결정된다. 즉, 다른 선택지가 없다. 이는 장점이자 단점이 될 수 있는데, 선택지가 없기 때문에 이후 캐시에서 블록을 찾을 때도 별도의 탐색 없이 바로 블록을 찾을 수 있다. 하지만 공간이 굉장히 많이 남아있음에도 블록을 교체해야 하는 상황이 발생할 수 있다. 블록의 교체는 하위 계층 메모리의 접근을 수반하므로, 비용이 꽤 크게 발생한다.

![](/imgs/ca/ca3.png)

위의 그림을 자세히 보면, 캐시 내 블록 주소는 메모리 블록 주소의 하위 3비트와 같음을 확인할 수 있다. 이는 우연일까? 물론 그렇지 않다. 대부분의 직접 매핑 캐시는 다음과 같은 방식으로 캐시 내 주소를 결정한다.

$$\text{Addr} = \text{(BlockAddress) } mod \text{ (NumofBlocks}, 2^n)$$

여기서 캐시의 블록 수는 $2^3$이므로, 주어진 주소에 대해 하위 3비트를 뜯어내는 것이다. 

당연하게도, 한 캐시 블록엔 여러 메모리 블록이 들어올 수 있다. 그러면 해당 블록이 우리가 찾는 블록이 맞는지 어떻게 확인할까? `Tag`와 `Valid`를 사용함으로써 해결할 수 있다. 태그(tag)는 주소 결정에 사용되지 않은 비트(`Index`)를 제외한 상위 비트로, 인덱스와 태그를 사용하면 메모리 주소가 하나로 결정된다. 또, 해당 영역이 비어있는지 아닌지의 확인도 필요하다. 유효(valid) 비트를 사용하여 캐시에 데이터가 있는지(1) 아닌지(0) 표시 및 확인할 수 있다.

### Cache Example
위의 그림으로부터, 메모리 주소가 `10101`인 블록을 캐시한다고 생각해보자. 캐시 블록 수가 8이므로, 인덱스는 하위 3비트인 `101`이 되고, 태그는 나머지 상위 비트인 `10`이 된다. 표로 나타내면 다음과 같다.

Index | Valid | Tag | Data
---|---|---|---
... | ... | ... | ...
100 | 0 | | 
101 | 1 | 10 | Mem\[10101]
... | ... | ... | ...

여기서 메모리 주소가 `10100`인 블록을 참조하려고 하면? Valid가 0이기 때문에 캐시 미스를 발생시키고, 하위 메모리로부터 데이터를 가져올 것이다. 즉, 블록 `10100`을 캐시하게 된다.

Index | Valid | Tag | Data
---|---|---|---
... | ... | ... | ...
100 | 1 | 10 | Mem\[10100] 
101 | 1 | 10 | Mem\[10101]
... | ... | ... | ...

이제 메모리 주소가 `11101`인 블록을 읽어보자. 캐시에서 `101` 위치는 valid하지만 태그가 일치하지 않으므로 이 또한 캐시 미스를 발생시키고, 블록 `11101`로 해당 영역을 교체할 것이다.

Index | Valid | Tag | Data
---|---|---|---
... | ... | ... | ...
100 | 1 | 10 | Mem\[10100] 
101 | 1 | 11 | Mem\[11101]
... | ... | ... | ...

### Address Subdivision
우리는 주어진 메모리 주소를 상위의 태그와 하위의 인덱스로 나누었었다. 조금 더 자세히 주소를 분할 방식을 알아보자. 

![](/imgs/ca/ca4.png)

인덱스를 위한 비트 수($n$)는 캐시 블록의 수($2^n$)에 대응된다. 나머지 상위 비트는 태그 비트가 된다. Byte offset은 주어진 블록에서의 변위를 지정하기 위해 쓰인다. 블록 내에서 0-3 바이트를 이동할 수 있으니, 달리 말하면 각각의 블록은 4바이트(1워드)의 크기를 가지고 있음을 알 수 있다. 주소 분할 방식만 봐도 캐시 구조의 일면을 볼 수 있는 셈이다. 연습 겸 예시를 하나 보자.

> 각 블록은 16바이트 크기고, 캐시는 64개의 블록을 가진다. 32비트 주소를 사용한다고 가정했을 때, 메모리 바이트 주소가 1200인 블록은 어디에, 어떻게 저장될까?

우선 주소를 적절하게 분할해보자. $2^4 = 16$바이트 크기이므로 바이트 오프셋은 총 4비트가 필요하다. 캐시 블록 수는 $2^6 = 64$개이므로, 인덱스엔 6비트를 할당해주자. 나머지 22비트는 태그 비트가 된다.

![](/imgs/ca/ca5.png)

바이트 주소가 1200이면, 블록 주소는 $\lfloor\frac{\text{AddrByte}}{\text{BlockSize}}\rfloor = \lfloor\frac{1200}{16}\rfloor =75$다. 캐시 내 블록 주소는 $75 \text{ mod } 64 = 11$이다. 즉, 001011이 인덱스로, 00...001이 태그로 들어간다.

### Block Size Considerations
블록 사이즈는 어떻게 정해야할까? 답은 '적당히'다.

큰 블록 사이즈는 공간적 지역성에 의해 캐시 미스의 빈도를 줄여준다. 그 이유를 예시 하나를 통해 생각해보자. 어떤 적당히 큰 배열 A가 있고, 배열의 원소 4개의 크기만큼을 블록 사이즈로 정했다고 가정한다. 그 배열에 대해 for-loop 등으로 순차적인 탐색을 한다면, 아마 원소 4개를 탐색할 때마다 배열 원소 일부를 캐시할 것이다. 최소 3번은 캐시 히트를 보장하는 셈이다. 만약 블록 크기가 원소 8개의 크기라면? 7번은 캐시 히트를 보장해줄 것이다.

하지만 여기엔 문제가 있는데, 증가된 블록 사이즈만큼 캐시 내의 블록 수는 줄어든다. 이는 같은 위치에 캐시될 블록 간 경쟁을 증가시켜 어느 시점부턴 오히려 캐시 미스 빈도(miss rate)가 증가하게 된다. 또, 큰 블록 사이즈는 과도한 prefetching으로 인한 pollution을 발생시킨다. 위의 예시에서, `A[0]`만을 사용하려고 했는데 `A[0]`부터 `A[7]`까지 모두 캐시하는 것은 분명한 낭비다. 모든 원소를 쓴다는 보장도 없고.

또, 많은 양의 캐시는 그 자체로 더 오래 걸린다. 큰 사이즈의 블록으로 얻어낸 이득을 여기서 잃을 수 있는 셈이다. 하지만 이는 해결할 방법이 몇 있다. 블록 내에서 필요한 영역부터 우선적으로 전송하는 방법(critical-word-first)과 블록 전체를 기다리지 않고 요청된 영역 도착 시 바로 실행하는 방법(early restart) 등이 있다. 

### Cache Miss
캐시 히트(hit) 시엔 별 문제가 없지만, 캐시 미스(miss)가 발생하면 메모리로부터 요청 데이터를 가져오고 다시 요청토록 해야 한다. 이를 위해 CPU 파이프라이닝을 어느 정도 지연시켜야 하고(메모리 읽기는 상대적으로 오래 걸린다), 하위 계층으로부터 데이터를 읽어야 하며, 이후 캐시 미스가 발생한 명령을 재시작해야 한다.

### Writing
메모리로의 쓰기는 크게 두 가지 방법이 있다. **Write-through**와 **Write-back**인데, 하나하나 알아보자.

#### Write-Through
**Write-through**는 쓰기 수행 시 캐시와 그에 대응되는 메모리 양 쪽에 쓰는 방식이다. 다만 메모리로의 쓰기는 굉장히 느리기 때문에, 이러한 방식은 저장(store)을 위한 명령어의 사이클을 엄청나게 증가시킨다.

이는 쓰기 버퍼(write buffer)를 이용해 어느 정도 완화할 수 있다. 쓸 내용을 버퍼에 저장하고 프로세서는 하던 일을 계속 한다. 그동안 버퍼에 있는 내용을 메모리에 쓰면 된다. 물론 이는 버퍼가 계속 꽉 차는 상황이 발생하면 난감해진다. 연속적으로 쓰기를 수행하면 지연이 발생할 것이다. 또, 만약 쓰기를 처리하는 속도가 버퍼보다 빠르다면 안쓰는 것만 못하다.

#### Write-Back
**Write-back**은 쓰기 수행 시 캐시에만 쓰고, 이후에 해당 블록이 다른 블록으로 교체될 때만 메모리에 쓰기를 수행하는 방식이다. 즉, 한 블록에 대해 여러 번 쓰기를 수행해도 메모리에는 한 번만 쓰면 되는 셈이다.

우리는 캐시에 `Dirty` 비트를 추가함으로써 이를 구현할 수 있다. 각 블록에 쓰기가 수행되어 교체 시 해당 내용을 메모리에 써야 함을 이 비트로 알린다. 만약 비트가 0이라면 별 작업 없이 블록 내용을 밀어버려도 괜찮다.

#### Write Allocation
앞서 말했듯, 쓰기 수행을 위해선, 우선 블록이 캐시 내에 있는지부터 확인해야 한다. 히트된다면 별 문제 없지만 미스 시 생각해봐야 할 문제가 있다. 

Write-through에서 캐시 미스 시엔 어떻게 해야 할까? 블록을 가져오는 방법(write allocation)과 블록을 가져오지 않고 메모리만 갱신하는 방법(write around)이 있다. 전자는 충분히 납득할 만 하지만, 후자는 어떻게 보면 굉장히 비효율적으로 보인다. 쓰고 나서 이후의 읽기에서 캐시 미스를 발생시키기 때문이다. 하지만 데이터의 초기화(initialization)같은 상황에서는 이 방법이 괜찮을 수도 있다. 프로그램 시작 부분에서 변수 초기화가 몰려있다면, 전자의 방식은 분명한 캐시의 낭비기 때문이다.

Write-back의 경우, 캐시 미스를 확인했을 때 교체해야 하는 영역이 dirty하다면(갱신해야 한다면), 원래 블록을 메모리에 쓴 뒤 교체해주어야 한다. 그렇지 않으면 원래 블록을 날려먹을 수 있기 때문이다. 다만 그렇게 되면 예전 블록을 쓰기 위한 시간만큼 지연이 발생하는데, 우리는 쓸 내용을 버퍼에 저장하고 실행을 이어감으로써 이를 해결할 수 있다.

## Cache Performance
이제 캐시가 어떻게 동작하는지는 대충 알아본 것 같다. 이제 성능에 관한 이야기를 해보자.

### Miss Panelty
우선 다음과 같은 상황을 가정한다.

DRAM이 메인 메모리로 사용되고, 이는 고정 width에 고정 주기의 버스 클락(bus clock)을 가진 버스에 연결되어 있다. 주소 전달에 1 버스 사이클이, DRAM 액세스에 15 버스 사이클이, 데이터 전송에 1 버스 사이클이 걸린다. 블록은 16 바이트 크기를 가지며, DRAM은 4 바이트의 width를 가진다. 
{:.success}

이러한 상황에서 캐시 미스에 의한 패널티는 얼마나 클까? 다음과 같은 방식으로 구할 수 있을 것이다.

$ \text{(MissPenalty)} = \text{(AddrTransfer)} + \text{(Access)} * \frac{\text{(Words)}}{\text{(WordWide)}} + \text{(DataTransfer)} * \text{(Words)} $

즉, 캐시 미스 패널티는 $ 1 + 4 * 15 + 4 * 1 = 65$ 버스 사이클이다. 그리고 사이클 당 전송 바이트, 즉, 대역폭(bandwidth)은 $\frac{16}{65} = 0.25$다.

이제 이를 개선하는 방법에 대해 가볍게 알아보도록 하자. 적당한 방법 중 하나는 메모리와 버스의 width를 넓히는 방법이고, 또 하나는 다수의 메모리 뱅크(memory bank)를 이용해 인터리빙(interleaving)하는 방법이다.

우선, 메모리와 버스의 width를 16 바이트로 넓히면 어떻게 될까? DRAM 액세스와 전송은 한 번만 발생하면 된다. 즉, $ 1 + 1 * 15 + 1 * 1 = 17$버스 사이클이다. 대역폭은 그에 따라 $0.94$가 된다.

4개의 메모리 뱅크에 의한 인터리빙을 적용한다면 어떻게 될까? 폭은 그대로이므로 데이터 전송 시간을 줄여주진 못하지만, 액세스는 한 번에 병렬적으로 일어나기 때문에 $ 1 + 1 * 15 + 1 * 4 = 20$버스 사이클을 요구한다. 대역폭은 $0.8$이다. 

인터리빙에 대한 내용은 나중에 따로 정리할 예정이다. 여기서 커버하는 내용은 아닌 듯해서...

### CPU Time
간단하게, CPU 타임을 다음과 같이 생각하자.

$ \text{(CPUtime)} = (\text{(CPUcc)} + \text{(MemoryStallcc)}) * \text{(CC time)} $

Memory stall cycle은 다음과 같이 나타낼 수 있다.

$ \text{(MemStallCC)} = \text{(TotalAccess)} * \text{(MissRate)} * \text{(MissPenalty)} $

충분히 납득할 수 있는 식이다. 이제 예시를 통해 성능을 대략적으로나마 분석해보자.

명령어 캐시 미스(I-cache miss)의 비율은 2%, 데이터 캐시 미스(D-cache miss)의 비율은 4%다. 미스 패널티는 동일하게 100 사이클이고, base CPI, 즉, 이상적인 경우의 CPI는 2 사이클이다. 그리고 Load/Store는 전체 명령어의 36%만큼 실행되고, 읽기와 쓰기의 성능 차이는 고려하지 않는다.
{:.success}

I-캐시의 경우, 명령어 당 $0.02 * 100 =2$, D-캐시의 경우 $0.04 * 0.36 * 100 = 1.44$만큼의 미스 패널티가 있다. 달리 말하면, 미스 패널티를 모든 명령어가 똑같이 나눠가지면 각각 $2 + 1.44 = 3.44$ 사이클을 가지게 된다는 것이다. 즉, CPI는 $2 + (2 + 1.44) = 5.44$로, 이상적인 경우에 비해 2.72배 더 오래 걸린다.

CPI 말고도 캐시 성능을 확인하기 위한 지표가 있다. **바로 평균 메모리 접근 시간(AMAT, Average Memory Access Time)**을 이용하는 것인데, 이것은 메모리 접근 연산 내에서만 고려한다. 즉, 메모리 접근 명령어의 비율 등을 따지지 않는다.

$ \text{AMAT} = \text{(HitTime)} + \text{(MissRate)} * \text{(MissPenalaty)} $

연습용으로 예시를 하나 생각해보자.

CPU는 1ns 주기의 클럭을 가지고, 히트 타임은 1 사이클, 미스 패널티는 20 사이클이다. 또, 캐시 미스 비율은 5%다.
{:.success}

이 때의 AMAT은 $ 1 + 0.05 * 20 = 2 $사이클이고, 이는 2ns에 해당한다. 이러한 측면에서 보면, 히트 타임도 어느 정도 중요하다. 어떠한 메모리 구조는 미스 비율을 줄이지만 히트 타임을 증가시키는데(e.g. n-way associated), 이 둘 사이의 적절한 타협점을 찾아야 한다.

## Associative Caches
직접 매핑 캐시와 상반...까진 아니고 어느 정도 대립하는 캐시 구조다. 앞에서도 잠깐 언급했지만, **연관(associative)** 되었다는 것은 일정 수의 블록이 집합을 이루어, 매핑되는 집합 내에선 어디에든 캐시될 수 있음을 의미한다. 위치를 결정하기 위한 블록 주소(혹은 블록 번호)는 직접 매핑과 유사한 방식으로 사용되지만, 이는 집합의 위치(혹은 번호)만을 결정하는 데 사용된다. 즉, 데이터가 들어갈 집합은 $\text{(Block Number) } mod \text{ (Number of Set)}$과 같다.

캐시 블록 수보다 작은 n에 대해, 연관 캐시는 n-way set associative와 fully associative, 두 종류로 나누어 볼 수 있다.

- **n-way set associative**: 각 집합은 n개의 엔트리(블록)를 가진다. 특정 집합에 매핑되는 블록은 n개의 위치에 존재할 수 있다.
- **Fully associative**: 캐시는 단일 집합으로 구성되어, 모든 블록은 캐시 내의 어디에든 위치할 수 있다.

![](/imgs/ca/ca6.png)

이러한 관점에서, 직접 매핑 캐시는 1-way associative한 캐시로 볼 수도 있다. 연관 정도에 따른 캐시 구조는 다음과 같이 나타내어진다.

![](/imgs/ca/ca7.png)

메모리 읽기를 시도할 때는, 주소에 의해 지정된 집합 내의 모든 엔트리를 찾는다. 연관 정도(associativity)가 클 수록 검색해야 하는 엔트리가 많아지는 셈이다.

![](/imgs/ca/ca8.png)

분명히 높은 연관 정도는 캐시 미스 비율을 낮춰주지만, 동시에 검색해야 하는 엔트리 수를 늘리기 때문에 히트 타임을 증가 시킨다. 다수의 comparator를 이용해 병렬적으로 검색하면 보다 빠르지만, 이는 또다른 비용을 낳는다. 또, 연관 정도를 두 배로 늘린다고 해서 미스 비율이 절반으로 줄어드는 것도 아니기 때문에, 용도에 따른 적절한 타협점을 찾아야 한다. VM 같은 경우는 무조건 미스 비율을 줄이는 게 좋기 때문에 완전 연관을 사용하는 것도 고려해 볼 수 있을 것이다. (VM 파트에서 다시 다루어보자)

## Replacement Policy
캐시 미스가 발생했을 때는 반드시 하위 계층으로부터 블록을 가져와야 한다. 직접 매핑 캐시의 경우 선택지가 없지만, 연관된 캐시라면 몇 가지의 선택지가 있다.

우선, 빈 엔트리, 즉 non-valid한 엔트리를 우선시한다. 만약 빈 엔트리가 없다면, 기존에 캐시되어 있던 엔트리 중 하나를 빼야 한다. 엔트리를 고르는 방법도 여러 가지가 있다.

- **LRU(Least-Recently Used)**: 최근에 사용되었음을 알리는 상태 비트를 추가하여, 잘 사용되지 않은 오래된 엔트리를 우선적으로 교체한다. 이는 연관 정도가 클수록 구현하는 데 어려움이 있다.
- **Random**: 무작위로 교체시킬 블록을 결정한다. 높은 연관 정도에선 LRU와 어느 정도 비슷한 성능을 보여준다.

## Multilevel Caches
캐시의 사용에서 가장 치명적인 부분은 시간이 오래 걸리는 메인 메모리로의 접근이다. 메인 메모리로의 접근을 줄이기 위한 또다른 방법으로, 우리는 여러 계층의 캐시를 사용할 수 있다. 즉, 2차, 3차 캐시를 도입하여 원래의 1차(primary) 캐시와 메인 메모리 사이에 놓는 것이다. 물론, 이들은 1차 캐시보다 느리지만, 그래도 메인 메모리보다는 훨씬 빠르고 용량도 크다. 

추가적인 캐시를 사용하는 경우, 1차 캐시에서 미스가 발생해도, 이후의 캐시에서 히트가 발생하면 메인 메모리로의 접근은 하지 않기 때문에 비용이 적게 든다. 물론, 최종 캐시에서 미스가 발생했을 땐, 더 많은 과정을 거쳤기에 비용이 조금 더 많이 든다. 하지만 그럼에도 다단계 캐시의 사용은 꽤 효과적이다. 다음 예시를 통해 성능을 비교해보자.

Base CPI가 1 사이클이고 clock rate가 4GHz(사이클 당 0.25ns)인 환경을 가정한다. 명령어 당 미스 비율은 2%, 메인 메모리 접근 시간은 100ns다.
{:.success}

단일 캐시에서는, 미스 패널티는 $\frac{100}{0.25} = 400$사이클이고, 유효 CPI는 $1 + 0.02 * 400 = 9$사이클이다. 이제 다음과 같은 2차, L-2 캐시를 생각해보자.

2차 캐시로의 접근 시간은 5ns다. Global miss rate, 즉, 모든 단계에서 캐시 미스일 확률은 0.5%다. 
{:.success}

이러한 가정은 정성적으론 꽤 납득할 만하다. 2차 캐시가 충분히 크다면 메인 메모리에서의 미스 비율을 낮춰줄 것이기 때문이다. 정량적인 디테일은 잘 모르겠다. 책의 예시를 그대로 가져왔으니 난 Patterson씨를 믿는다.

아무튼, 여기서 1차 캐시만 미스가 발생할 경우 미스 패널티는 $ 5/0.25 = 20$사이클이다. 2차 캐시까지 미스가 발생할 경우엔 2차에서 메인 메모리에 접근한 뒤, 이를 가져와 1차가 2차로의 캐시 읽기를 재시도하도록 해야 한다. 앞에서의 400 사이클보다 더 많이 비용이 발생하는 셈인데, 대충 500 사이클이라 하자. 그렇다면, CPI는 $ 1 + 0.02 * 20 + 0.005 * 500 = 3.9$사이클이다. 1차 캐시만 사용하는 경우보다 훨씬 비용이 적게 든다.

다단계 캐시를 사용할 땐 고려해야 할 점이 있다. 1차 캐시는 히트 타임을 줄이는 걸 목표로 하는 게 좋다. 미스 시에도 패널티가 상대적으로 적고, 히트 타임을 줄여 프로그램의 전반적인 클럭 사이클 단축과 파이프라이닝의 축소를 도모하는 게 이득이기 때문이다. 반면 2차 캐시는 미스 패널티가 굉장히 크므로 미스 비율을 줄이는 방향으로 설계되는 게 좋다.

이러한 사항을 고려하여 다단계 캐시를 구현하면, 단일 캐시에 비해 1차 캐시는 사이즈가 작고, 블록 크기도 작아진다. 2차 캐시는 보다 연관되어 있고, 그 크기 자체도 커지게 된다.

## Algorithms for Cache
캐시 성능의 향상을 위해선, 프로그램도 캐시 성능을 고려하여 설계되어야 한다. 캐시는 지역성을 기반으로 설계되었기 때문에, 좋은 지역성을 가지면 좋은 캐시 성능을 보일 거란 점은 앞에서 언급했다. 

또, 블록화이란 방법이 있다. 어떤 배열이나 행렬 등을 처리할 때, 전체를 처리하기보단 부분배열, 부분행렬을 따와 블록화하여 이를 블록 단위로 처리하는 알고리즘이다. 이는 캐시된 블록이 버려지기 전에 최대한 사용함으로써 미스 비율을 낮추고 시간적 지역성을 확보하는 방법이다.

물론, 대부분의 경우 똑똑한 컴파일러가 좋은 캐시 성능을 이끌어내기 위해 최적화를 해줄 것이다. 다만 optimization blocker가 없는지 확인을 잘 해주도록 하자.

## CPUs for Cache
명령어는 순차적으로 처리될 것으로 보이지만, 그렇지 않을 수도 있다. 이는 분기나 점프같은 경우를 말하는 게 아니라, 여러 명령어들의 순서가 뒤바뀔 수 있음을 의미한다. 의존성(dependancy)이 없는 명령어들이라면 그렇게 해도 괜찮을 것으로 보인다. 이러한 방식을 비순차적 명령어 처리(out-of-order execution)라고 한다.

이런 비순차적 처리를 지원하는 CPU는 비블로킹 캐시(nonblocking cache) 구조를 사용하여, 캐시 미스에 의한 패널티를 감출 수 있다. 캐시 미스 발생 시 다른 작업을 통해 패널티를 어느 정도 줄인 것처럼 보이게 하거나, 여러 캐시 미스를 중첩시켜 총 패널티가 줄어든 것처럼 보이게 할 수 있다. 물론 이러한 방식은 병렬적인 처리가 가능한 설계 및 충분한 대역폭을 요구한다. 이 내용은 나중에 다시 짚어보도록 하자.

# Other Issues
## 3C: Sources of Misses

- Compulsory miss: cold start miss라고도 불린다. 블록에 대한 최초 접근 시에 발생하는 필연적인 캐시 미스다.
- Capacity miss: 제한된 캐시 크기로 인해 발생하는 캐시 미스다. 
- Conflict miss: 블록들이 하나의 집합 혹은 위치에 대해 결쟁을 벌일 때 발생하는 캐시 미스다.

우리는 이들 사이의 적절한 타협을 통해 특정 캐시 미스의 비율이나 비용을 줄여볼 수 있다. 예를 들어, 연관도를 높이면 conflict miss의 비율이 줄어들 것이다. 하지만 이는 높은 구현 비용이나 높은 접근 시간을 초래하므로, 오히려 전체 성능이 나빠질 수 있다. 다양한 요인들을 서로 절충하고, 이들의 상호작용을 고려하는 게 캐시 설계의 주요한 어려움이라 할 수 있겠다.

Design | Effect | Possible negative effect
---|---|---
캐시 크기 증가 | Capacity miss의 비율이 감소한다. | 접근 시간을 증가시킨다.
연관도 증가 | Conflict miss의 비율이 감소한다. | 접근 시간을 증가시킨다.
블록 크기 증가 | 공간적 지역성에 의해 미스 비율이 줄어든다. | 미스 비용이 커지고, pollution을 발생시킬 수 있다.


## Cache Coherence Problem
멀티 코어, 혹은 멀티 프로세서 시스템에서, "어느 프로세서에서 접근하든 같은 위치로의 접근은 같은 결과를 내놓아야 한다"는 원칙이 지켜져야 한다. 캐시는 프로세서 각각에 할당되기 때문에, 그러한 일관성(coherence)을 지키기 위한 별도의 노력이 필요하다.

Write-back 방식이 적용된 캐시에서, CPU A와 B 모두 어떤 메모리 영역에 대한 블록이 캐시되어 있다고 하자. 이 때, A가 그 블록에 데이터를 쓴다고 해도, write-back 방식이므로 쓰기의 결과가 메모리에 당장 적용되지는 않는다. 그런데 이 시점에서 B가 그 블록을 읽는다면? B는 수정되지 않은 old-value를 읽게 된다. 일관성이 보장되지 않는 경우다.

이렇게 멀티 프로세서에서 일관성 문제를 해결하기 위한 규약(protocol)들이 있다. 스누핑(snooping) 프로토콜, 디렉터리 기반(directory-based) 프로토콜이 대표적인 예시다. 스누핑 프로토콜은 버스 위에서의 읽기 및 쓰기 동작을 감시함으로써 행해지고, 디렉터리 기반 프로토콜은 블록의 공유 상태를 디렉터리에 기록하는 방식으로 이루어진다.

여기선 스누핑 프로토콜, 그 중에서도 쓰기 무효화(invalidating) 스누핑 프로토콜을 중심으로 알아볼 것이다.

### Invalidating Snooping Protocol
쓰기 무효화 프로토콜은 캐시 블록에 대한 배타적인 접근을 보장하는 방식으로 일관성을 유지한다. 예를 들어, 어떤 블록 X가 캐시 A와 B 위에 올라왔을 때, 프로세서 A가 자신의 캐시에 쓰기를 수행했다면, 캐시 B에 올라온 블록 X를 무효화한다. 그러면 이후에 B가 X를 읽으려 하면 캐시 미스가 발생하고, A가 메모리를 비롯한 관련 블록을 모두 갱신한다.

# Exercises
## Direct Mapped Cache 1
32비트 메모리 주소 체계에서, 다음과 같은 워드 주소로 순차적 메모리 접근을 시도한다.

3, 180, 43, 2, 191, 88, 190, 14, 181, 44, 186, 253

단일 워드 블록 16개로 구성된 직접 사상 캐시에서, 각 참조의 주소, 태그, 인덱스를 보이자. 또, 캐시가 처음에 비었다고 가정하고, 각 참조에 대한 히트/미스 여부를 나타내자.
{:.success}

주소가 256을 넘지 않으므로, 주소는 하위 8비트만 생각하자. 우선, 캐시 구조를 생각해야 한다.

- 캐시의 블록 수는 $2^4 = 16$개이므로, 주어진 주소의 하위 4비트가 캐시의 블록 주소를 결정한다. 즉, 인덱스는 하위 4비트다.
- 나머지 상위 비트들은 해당 블록이 주소와 일치하는지 확인하기 위한 태그로 사용된다. 여기선 상위 4비트만 생각하자.

Decimal | Binary | Tag | Index | Hit/Miss
---|---|---|---|---
3 | 0000 0011 | 0000 | 0011 | Miss (빈 캐시)
180 | 1011 0100 | 1011 | 0100 | Miss (빈 캐시)
43 | 0010 1011 | 0010 | 1011 | Miss (빈 캐시)
2 | 0000 0010 | 0000 | 0010 | Miss (빈 캐시)
191 | 1011 1111 | 1011 | 1111 | Miss (빈 캐시)
88 | 0101 1000 | 0101 | 1000 | Miss (빈 캐시)
190 | 1011 1110 | 1011 | 1110 | Miss (빈 캐시)
14 | 0000 1110 | 0000 | 1110 | Miss (태그 불일치)
181 | 1011 0101 | 1011 | 0101 | Miss (빈 캐시)
44 | 0010 1100 | 0010 | 1100 | Miss (빈 캐시)
186 | 1011 1010 | 1011 | 1010 | Miss (빈 캐시)
253 | 1111 1101 | 1111 | 1101 | Miss (빈 캐시)

2워드 블록 8개로 구성된 직접 사상 캐시에 대해 위의 과정을 다시 해보자.
{:.success}

- 2워드 블록이므로, 하위 1비트는 오프셋으로 둔다.
- 캐시 믈록 수가 $2^3=8$개이므로, 인덱스는 오프셋 다음 하위 3비트다.
- 나머지 상위 4비트는 태그로 사용된다.

Decimal | Binary | Tag | Index | Hit/Miss
---|---|---|---|---
3 | 0000 0011 | 0000 | 001 | Miss (빈 캐시)
180 | 1011 0100 | 1011 | 010 | Miss (빈 캐시)
43 | 0010 1011 | 0010 | 101 | Miss (빈 캐시)
2 | 0000 0010 | 0000 | 001 | Hit
191 | 1011 1111 | 1011 | 111 | Miss (빈 캐시)
88 | 0101 1000 | 0101 | 100 | Miss (빈 캐시)
190 | 1011 1110 | 1011 | 111 | Hit
14 | 0000 1110 | 0000 | 111 | Miss (태그 불일치)
181 | 1011 0101 | 1011 | 010 | Hit
44 | 0010 1100 | 0010 | 110 | Miss (빈 캐시)
186 | 1011 1010 | 1011 | 101 | Miss (태그 불일치)
253 | 1111 1101 | 1111 | 110 | Miss (태그 불일치)

결론적으로, 우리는 다음과 같은 사실을 알 수 있다. 직접 사상 캐시에서,

- 어떤 바이트/워드 주소 체계에서, 캐시 블록 사이즈가 $2^n$ 바이트/워드라면, 하위 $n$비트는 오프셋이다.
- 캐시 블록의 수가 $2^n$개라면, 오프셋 다음의 하위 $n$비트는 인덱스다.
- 나머지 상위 비트들은 태그가 된다.

## Direct Mapped Cache
32비트 바이트 주소 체계에서, 31-10비트 필드가 태그를, 9-5비트 필드가 인덱스를, 그리고 4-0비트 필드가 오프셋을 지정한다. 여기서 캐시 블록의 크기, 캐시 엔트리의 개수를 구하자.
{:.success}

오프셋이 5개의 비트를 차지하므로 캐시 블록은 $2^5 = 32$바이트, 즉 8워드의 크기를 가진다. 또, 인덱스가 5개의 비트를 차지하므로 총 $2^5 = 32$개의 블록을 가지고 있다.

## Multi-level Cache
두 프로세서에, P1, P2는 각각 다음과 같은 스펙의 L1 캐시를 가진다. 캐시의 크기가 클수록 히트 타임이 길어지는 경향이 고려되었다. 메인 메모리 접근 시간은 70 ns고, 전체 명령어의 36%가 메모리 접근 명령어다. 

X | L1 크기 | L1 미스 비율 | L1 히트 타임
---|---|---|---
P1 | 2 KiB | 8.0% | 0.66 ns
P2 | 4 KiB | 6.0% | 0.90 ns


L1의 히트 타임이 프로세서의 사이클 시간을 결정하며, 메모리 접근에 의한 지연이 없을 때의 CPI가 1.0이라 가정한다. 각 프로세서의 클럭 속도와 AMAT, 그리고 전체 CPI를 구해보자.
{:.success}

사이클 시간이 L1 히트 타임과 같고, 클럭 속도(clock rate)는 그것의 역수다. 즉, P1은 1.51 GHz, P2는 1.11 GHz의 클럭 속도를 가진다.

$\text{AMAT} = \text{(hit time)} + \text{(Miss rate)} * \text{(Miss penalty)}$

AMAT을 계산하기 전, 사이클에 관해 먼저 생각해보자. P1과 P2의 메모리 접근은 각각 107, 78 사이클이 소요된다. 프로세서는 사이클 중간에 멈추거나 그 전에 끝내 다음 작업으로 넘어갈 수 없다. 그렇기 때문에 $\text{(MemAccess)} / \text{(CycleTime)}$에서 소수점 올림을 해주어야 한다. 즉, 위의 AMAT 공식을 적용하면,

- P1: $(1 + 0.08 * 107) * 0.66 = 9.56 cycles * 0.66 ns/cycle = 6.31 ns$
- P2: $(1 + 0.06 * 78) * 0.90 = 5.68 cycles * 0.90 ns/cycle = 5.68 ns$

이제 CPI를 계산해보자. 전체 CPI는 어떠한 패널티를 모두가 나눈다고 생각하고 계산하면 편하다. 여기서, 우리는 명령어에 대한 캐시 미스와 메모리 접근에 대한 캐시 미스를 모두 고려해야 한다. 명령어 또한 메모리에서 가져오기 때문이다. 별다른 언급이 없으면 동일한 실패율을 가진다고 보면 된다.

명령어에 대한 캐시 미스를 우선 생각해보자. 모든 명령어에 대해 적용되니 $1.0 * \text{(miss rate)} * \text{(miss penalty)}$만큼 CPI가 추가된다. 즉, P1은 $0.08 * 107 = 8.56$, P2는 $0.06 * 78 = 4.68$사이클만큼 추가된다.

이제 메모리 접근에 대한 캐시 미스를 생각하자. 메모리 접근 명령어의 비율이 양쪽 다 $0.36$이므로, P1은 $0.36 * 0.08 * 107 = 3.0816$, P2는 $0.36 * 0.06 * 78 = 1.6846$사이클만큼 추가된다.

정리하면, P1의 CPI는 $1+8.56+3.0816=12.6416$이고, P2의 CPI는 $1+4.68+1.6848=7.3648$이 된다.

결론적으로, 프로세서 P2가 더 빠르다고 볼 수 있다.

위의 상황에서, P1에 L2 캐시를 도입하여 다단계 캐시(multi-level cache)를 구현하려 한다. L2 캐시의 스펙은 아래와 같고, 나머지 조건은 전부 동일하다. L2 미스 비율은 L2 캐시에서의 local miss에 대한 비율이다. 이 때, L2 캐시가 도입된 P1의 AMAT, CPI를 구하고, P2와 비교해보자. 그리고 느린 쪽의 프로세서가 빠른 쪽의 프로세서를 따라가기 위해 L1 캐시의 미스 비율을 어느 정도로 낮춰야 하는지도 확인해보자.
{:.success}

캐시 크기 | L2 미스 비율 | L2 히트 타임
---|---|---
1 MiB | 95% | 5.62 ns

우선, L2 캐시의 도입에 따라 미스 패널티가 수정되어야 한다. 우선 L1 실패 시 L2 히트 타임은 5.62 ns이므로, 히트 시 사이클은 ${5.62 \over 0.66} = 8.52 \to 9$다. 그리고 95%의 비율로 107사이클이 소요되는 메모리 접근을 해야 하므로, 미스 패널티는 $9 + 0.95 * 107 = 110.65$사이클이며, 이를 시간으로 환산하면 73.029 ns다. 즉, AMAT은 $0.66 + 0.08 * 73.029 = 6.50 ns$다.

비슷한 원리로 CPI를 구해보자. 미스 패널티가 110.65사이클인 점을 고려하면, CPI는 $1 + 0.08 * 110.65 + 0.36 * 0.08 * 110.65 = 13.03872$다.

전반적으로 L2가 없을 때보다 나쁜 성능을 보인다... 그럼 이러한 P1의 L1 미스 비율을 줄여 P2의 성능까지 올려보자! CPI 기준으로 알아보도록 한다.

다단계 캐시가 적용된 P1은 13.04의 CPI, 그리고 그에 따른 평균 8.61 ns의 명령어 실행 시간을 가진다. 반면 P2는 7.3648의 CPI, 그리고 그에 따른 평균 6.63 ns의 명령어 실행 시간을 가진다. 이러한 관계에 의하면, 개선된 P1의 CPI는 $\text{CPI} * 0.66 = 6.63 \to \text{CPI} = 10.04$로 줄어야 한다.

CPI를 구했던 공식을 다시 살펴보자. $\text{(CPI)} = 1 + \text{(miss rate)} * 110.65 + \text{(miss rate)} * 0.36 * 110.65$이므로, 개선된 미스 비율은 약 6%가 된다. 즉, 미스 비율을 6%까지 줄여야 P2의 성능을 따라갈 수 있다.


# 마치며
우리는 지금까지 캐시의 구조와 동작 방식을 알아보면서도, 어떻게 해야 좋은 성능을 이끌어낼 수 있을지에 대한 고민을 계속 해왔다. 연관이나 교체나 쓰기 방식이나, 모두 다 정확한 동작을 유지하면서도 좋은 성능을 이끌어내기 위한 노력의 일환이다. 컴퓨터 구조에서 성능 문제는 절대로 빼놓을 수 없는 셈이다.

아무튼 다음엔 가상 메모리(VM)와 메모리 계층 구조에 관한 나머지 주제들을 알아보려고 한다.

생각보다 포스트가 굉장히 길었다.
