---
title: "메모리 계층 구조 - 캐시"
tags: Computer-Architecture
toc: true
---

# Basic Principle
많은 CSE 과목에서 메모리 계층 구조(memory hierarchy)에 대한 개념을 한 번은 꼭 짚고 넘어간다. 지금도 한 세 번째는 우려먹는 것 같다. 아무튼 그래서 전반적인 내용은 알고 있다고 가정하고 정리하려 한다.

컴퓨터 내엔 여러 종류의 메모리가 있고, 이들은 용량, 접근 속도, 용도 등의 측면에서 서로 다르다. 예를 들어 SRAM은 빠르지만 용량 대비 굉장히 비싸고, 디스크는 가격 대비 매우 큰 용량을 지니고 있지만 무척 느리다. 또, 빠른 메모리일 수록 CPU와 (개념적으로) 가까이 있다.

![](/imgs/ca/ca2.png)

즉, 속도와 비용은 동시에 만족하기가 굉장히 어렵다. 이러한 한계를 극복하기 위한 시도 중 하나가 메모리 계층 구조를 활용하는 것이다.


# Locality
메모리 계층 구조를 논할 때, 지역성(locality)는 항상 빠지지 않는다. 왜냐? 메모리 계층 구조는 좋은 지역성을 가진 프로그램을 가정하고, 그에 맞추어 성능적인 측면에서의 이점을 가져가기 때문이다. 달리 말하면, 나쁜 지역성을 가진 프로그램의 경우, 좋은 지역성의 프로그램보다 나쁜 퍼포먼스를 보일 것이다.

즉, 우리는 메모리 계층 구조를 설계하고 이해할 때, 그것이 지역성을 잘 활용하여 좋은 성능을 이끌어내는지 확인해야 한다. 프로그램을 설계할 때도, (우리가 사용하는 컴퓨터는 지역성을 충분히 활용함을 알고 있으므로) 그것이 좋은 지역성을 가지는지 확인해야 한다.

지역성은 두 종류로 나눌 수 있다.

- 시간적 지역성(temporal locality): 최근에 액세스한 대상은 이후에 또 다시 액세스할 가능성이 크다.
- 공간적 지역성(spatial locality): 최근에 액세스한 대상 근처의 대상도 액세스될 가능성이 크다.

시간적 지역성의 대표적인 예시는 for loop의 `i` 등이 있고, 공간적 지역성의 대표적인 예시는 순차적인 명령어 접근이나 배열(array) 등이 있을 것이다.

    for (int i=0; i < n; i++){
   	    for (int j=0; j < n; j++){
    		Arr[j][i] = 0;
		}	
    }

위와 같은 코드는 공간적 지역성이 잘 지켜지지 않은 케이스다. 배열은 row-major하게 할당되는데, 저러한 방식은 주소를 계속 건너뛰면서 참조하기 때문이다. 


# Cache Memory
보통 캐시(cache)는 상대적인 개념으로, 낮은 계층의 데이터를 가져와 접근 속도를 높이는 방법 혹은 그러한 역할을 수행하는 계층을 의미한다. 캐시 메모리(cache memory)는 CPU와 가장 가까이 있어, 메인 메모리로부터 캐시 역할을 하는 메모리를 의미한다.

그 전에, 용어를 간단히 정리하고 가자.

## Terminologies
블록(block), 또는 라인(line)은 캐시에서의 복사본의 기본 단위다. 각각의 블록이 가지는 크기는 아키텍처마다 다를 수 있다. 하나의 워드만 포함할 수도, 몇 개의 워드를 포함할 수도 있다.

![](/imgs/ca/ca1.png)

우리는 낮은 계층의 메모리가 가진 데이터에 접근하려 할 때, 직접 그곳에 접근하기보단 캐시를 활용한다. 하지만 모든 데이터가 캐시 내에 있지는 않다. 찾던 데이터가 캐시에 있는 상황을 hit라 하고, 그렇지 않은 상황을 miss라 한다.

캐시 미스가 발생했을 때, 캐시는 낮은 계층으로부터 해당 데이터를 가져와야 한다. 이 때 걸리는 시간을 miss panelty라 하며, 캐시 미스가 발생하는 비율을 miss rate라 한다.

## Mapping
캐시는 하위 계층으로부터 데이터를 가져와 어떻게 저장할까? 또, 해당 데이터를 어떻게 찾을 수 있을까? 크게 세 방법으로 나눌 수 있다.

- Direct-mapped : 하위 계층의 주소를 이용해 캐시 내 주소를 유일하게 결정한다. (1-way associative)
- n-way associative : 캐시를 n개의 블록을 가지는 집합으로 분할하여, 하위 계층의 주소를 이용해 캐시 내 집합을 유일하게 결정한다.
- fully associative : 블록은 캐시 내 어디든지 위치할 수 있다.

각각은 장단점을 지니고 있다. 직접 매핑(direct-mapped) 캐시는 블록을 찾는 데 비용이 거의 들지 않지만, fully associative하다면 비용이 상당히 발생할 것이다. 반대로, 직접 매핑 캐시에서 같은 위치에 캐시되는 두 블록이 번갈아가며 참조되는 경우를 생각해볼 수 있을 것이다. 블록을 교체하는 과정이 계속 반복되는 셈이다. 연관된(associative) 캐시는 그러한 경쟁이 적기 때문에 이러한 점에선 효과적이다.

각각의 방법을 하나하나 알아볼 예정이다. 또, 직접 매핑 방식에서 캐시에 대한 전반적인 동작 과정을 알아보도록 하자.

## Direct-Mapped Cache & Cache Basics
캐시가 어떻게 작동하는지 알아보자. 이를 위해 우선 직접 매핑 캐시를 먼저 알아보고, 이를 기반으로 캐시의 동작 과정을 살펴볼 것이다.

직접 매핑 캐시에서는, 주어진 메모리 블록 주소에 의해 캐시 내 위치가 유일하게 결정된다. 즉, 다른 선택지가 없다. 이는 장점이자 단점이 될 수 있는데, 선택지가 없기 때문에 이후 캐시에서 블록을 찾을 때도 별도의 탐색 없이 바로 블록을 찾을 수 있다. 하지만 공간이 굉장히 많이 남아있음에도 블록을 교체해야 하는 상황이 발생할 수 있다. 블록의 교체는 하위 계층 메모리의 접근을 수반하므로, 비용이 꽤 크게 발생한다.

![](/imgs/ca/ca3.png)

위의 그림을 자세히 보면, 캐시 내 블록 주소는 메모리 블록 주소의 하위 3비트와 같음을 확인할 수 있다. 이는 우연일까? 물론 그렇지 않다. 대부분의 직접 매핑 캐시는 다음과 같은 방식으로 캐시 내 주소를 결정한다.

$$Addr = (BlockAddress) mod (NumofBlocks, 2^n)$$

여기서 캐시의 블록 수는 $2^3$이므로, 주어진 블록 주소에 대해 하위 3비트를 뜯어내는 것이다. 

당연하게도, 한 캐시 블록엔 여러 메모리 블록이 들어올 수 있다. 그러면 해당 블록이 우리가 찾는 블록이 맞는지 어떻게 확인할까? `Tag`와 `Valid`를 사용함으로써 해결할 수 있다. 태그(tag)는 주소 결정에 사용되지 않은 비트(`Index`)를 제외한 상위 비트로, 인덱스와 태그를 사용하면 메모리 주소가 하나로 결정된다. 또, 해당 영역이 비어있는지 아닌지의 확인도 필요하다. 유효(valid) 비트를 사용하여 캐시에 데이터가 있는지(1) 아닌지(0) 표시 및 확인할 수 있다.

### Cache Example
위의 그림으로부터, 메모리 주소가 `10101`인 블록을 캐시한다고 생각해보자. 캐시 블록 수가 8이므로, 인덱스는 하위 3비트인 `101`이 되고, 태그는 나머지 상위 비트인 `10`이 된다. 표로 나타내면 다음과 같다.

Index | Valid | Tag | Data
---|---|---|---
... | ... | ... | ...
100 | 0 | | 
101 | 1 | 10 | Mem\[10101]
... | ... | ... | ...

여기서 메모리 주소가 `10100`인 블록을 참조하려고 하면? Valid가 0이기 때문에 캐시 미스를 발생시키고, 하위 메모리로부터 데이터를 가져올 것이다. 즉, 블록 `10100`을 캐시하게 된다.

Index | Valid | Tag | Data
---|---|---|---
... | ... | ... | ...
100 | 1 | 10 | Mem\[10100] 
101 | 1 | 10 | Mem\[10101]
... | ... | ... | ...

이제 메모리 주소가 `11101`인 블록을 읽어보자. 캐시에서 `101` 위치는 valid하지만 태그가 일치하지 않으므로 이 또한 캐시 미스를 발생시키고, 블록 `11101`로 해당 영역을 교체할 것이다.

Index | Valid | Tag | Data
---|---|---|---
... | ... | ... | ...
100 | 1 | 10 | Mem\[10100] 
101 | 1 | 11 | Mem\[11101]
... | ... | ... | ...

### Address Subdivision
우리는 주어진 메모리 주소를 상위의 태그와 하위의 인덱스로 나누었었다. 조금 더 자세히 주소를 분할 방식을 알아보자. 

![](/imgs/ca/ca4.png)

인덱스를 위한 비트 수($n$)는 캐시 블록의 수($2^n$)에 대응된다. 나머지 상위 비트는 태그 비트가 된다. Byte offset은 주어진 블록에서의 변위를 지정하기 위해 쓰인다. 블록 내에서 0-3 바이트를 이동할 수 있으니, 달리 말하면 각각의 블록은 4바이트(1워드)의 크기를 가지고 있음을 알 수 있다. 주소 분할 방식만 봐도 캐시 구조의 일면을 볼 수 있는 셈이다. 연습 겸 예시를 하나 보자.

> 각 블록은 16바이트 크기고, 캐시는 64개의 블록을 가진다. 32비트 주소를 사용한다고 가정했을 때, 메모리 바이트 주소가 1200인 블록은 어디에, 어떻게 저장될까?

우선 주소를 적절하게 분할해보자. $2^4 = 16$바이트 크기이므로 바이트 오프셋은 총 4비트가 필요하다. 캐시 블록 수는 $2^6 = 64$개이므로, 인덱스엔 6비트를 할당해주자. 나머지 22비트는 태그 비트가 된다.

![](/imgs/ca/ca5.png)

바이트 주소가 1200이면, 블록 주소는 $\lfloor\frac{AddrByte}{BlockSize}\rfloor = \lfloor\frac{1200}{16}\rfloor =75$다. 캐시 내 블록 주소는 $75 mod 64 = 11$이다. 즉, 001011이 인덱스로, 00...001이 태그로 들어간다.

### Block Size Considerations
블록 사이즈는 어떻게 정해야할까? 답은 '적당히'다.

큰 블록 사이즈는 공간적 지역성에 의해 캐시 미스의 빈도를 줄여준다. 그 이유를 예시 하나를 통해 생각해보자. 어떤 적당히 큰 배열 A가 있고, 배열의 원소 4개의 크기만큼을 블록 사이즈로 정했다고 가정한다. 그 배열에 대해 for-loop 등으로 순차적인 탐색을 한다면, 아마 원소 4개를 탐색할 때마다 배열 원소 일부를 캐시할 것이다. 최소 3번은 캐시 히트를 보장하는 셈이다. 만약 블록 크기가 원소 8개의 크기라면? 7번은 캐시 히트를 보장해줄 것이다.

하지만 여기엔 문제가 있는데, 증가된 블록 사이즈만큼 캐시 내의 블록 수는 줄어든다. 이는 같은 위치에 캐시될 블록 간 경쟁을 증가시켜 어느 시점부턴 오히려 캐시 미스 빈도(miss rate)가 증가하게 된다. 또, 큰 블록 사이즈는 과도한 prefetching으로 인한 pollution을 발생시킨다. 위의 예시에서, `A[0]`만을 사용하려고 했는데 `A[0]`부터 `A[7]`까지 모두 캐시하는 것은 분명한 낭비다. 모든 원소를 쓴다는 보장도 없고.

또, 많은 양의 캐시는 그 자체로 더 오래 걸린다. 큰 사이즈의 블록으로 얻어낸 이득을 여기서 잃을 수 있는 셈이다. 하지만 이는 해결할 방법이 몇 있다. 블록 내에서 필요한 영역부터 우선적으로 전송하는 방법(critical-word-first)과 블록 전체를 기다리지 않고 요청된 영역 도착 시 바로 실행하는 방법(early restart) 등이 있다. 

### Cache Miss
캐시 히트(hit) 시엔 별 문제가 없지만, 캐시 미스(miss)가 발생하면 메모리로부터 요청 데이터를 가져오고 다시 요청토록 해야 한다. 이를 위해 CPU 파이프라이닝을 어느 정도 지연시켜야 하고(메모리 읽기는 상대적으로 오래 걸린다), 하위 계층으로부터 데이터를 읽어야 하며, 이후 캐시 미스가 발생한 명령을 재시작해야 한다.

### Writing
메모리로의 쓰기는 크게 두 가지 방법이 있다. Write-through와 Write-back인데, 하나하나 알아보자.

#### Write-Through
Write-through는 쓰기 수행 시 캐시와 그에 대응되는 메모리 양 쪽에 쓰는 방식이다. 다만 메모리로의 쓰기는 굉장히 느리기 때문에, 이러한 방식은 저장(store)을 위한 명령어의 사이클을 엄청나게 증가시킨다.

이는 쓰기 버퍼(write buffer)를 이용해 어느 정도 완화할 수 있다. 쓸 내용을 버퍼에 저장하고 프로세서는 하던 일을 계속 한다. 그동안 버퍼에 있는 내용을 메모리에 쓰면 된다. 물론 이는 버퍼가 계속 꽉 차는 상황이 발생하면 난감해진다. 연속적으로 쓰기를 수행하면 지연이 발생할 것이다. 또, 만약 쓰기를 처리하는 속도가 버퍼보다 빠르다면 안쓰는 것만 못하다.

#### Write-Back
Write-back은 쓰기 수행 시 캐시에만 쓰고, 이후에 해당 블록이 다른 블록으로 교체될 때만 메모리에 쓰기를 수행하는 방식이다. 즉, 한 블록에 대해 여러 번 쓰기를 수행해도 메모리에는 한 번만 쓰면 되는 셈이다.

우리는 캐시에 `Dirty` 비트를 추가함으로써 이를 구현할 수 있다. 각 블록에 쓰기가 수행되어 교체 시 해당 내용을 메모리에 써야 함을 이 비트로 알린다. 만약 비트가 0이라면 별 작업 없이 블록 내용을 밀어버려도 괜찮다.

#### Write Allocation
앞서 말했듯, 쓰기 수행을 위해선, 우선 블록이 캐시 내에 있는지부터 확인해야 한다. 히트된다면 별 문제 없지만 미스 시 생각해봐야 할 문제가 있다. 

Write-through에서 캐시 미스 시엔 어떻게 해야 할까? 블록을 가져오는 방법(write allocation)과 블록을 가져오지 않고 메모리만 갱신하는 방법(write around)이 있다. 전자는 충분히 납득할 만 하지만, 후자는 어떻게 보면 굉장히 비효율적으로 보인다. 쓰고 나서 이후의 읽기에서 캐시 미스를 발생시키기 때문이다. 하지만 데이터의 초기화(initialization)같은 상황에서는 이 방법이 괜찮을 수도 있다. 프로그램 시작 부분에서 변수 초기화가 몰려있다면, 전자의 방식은 분명한 캐시의 낭비기 때문이다.

Write-back의 경우, 캐시 미스를 확인했을 때 교체해야 하는 영역이 dirty하다면(갱신해야 한다면), 원래 블록을 메모리에 쓴 뒤 교체해주어야 한다. 그렇지 않으면 원래 블록을 날려먹을 수 있기 때문이다. 다만 그렇게 되면 예전 블록을 쓰기 위한 시간만큼 지연이 발생하는데, 우리는 쓸 내용을 버퍼에 저장하고 실행을 이어감으로써 이를 해결할 수 있다.

## Cache Performance
이제 캐시가 어떻게 동작하는지는 대충 알아본 것 같다. 이제 성능에 관한 이야기를 해보자.

### Miss Panelty
우선 다음과 같은 상황을 가정한다.

> DRAM이 메인 메모리로 사용되고, 이는 고정 width에 고정 주기의 버스 클락(bus clock)을 가진 버스에 연결되어 있다. 주소 전달에 1 버스 사이클이, DRAM 액세스에 15 버스 사이클이, 데이터 전송에 1 버스 사이클이 걸린다. 블록은 16 바이트 크기를 가지며, DRAM은 4 바이트의 width를 가진다. 

이러한 상황에서 캐시 미스에 의한 패널티는 얼마나 클까? 다음과 같은 방식으로 구할 수 있을 것이다.

$ (MissPenalty) = (AddrTransfer) + (Access) * \frac{(Words)}{(WordWide)} + (DataTransfer) * (Words) $

즉, 캐시 미스 패널티는 $ 1 + 4 * 15 + 4 * 1 = 65$ 버스 사이클이다. 그리고 사이클 당 전송 바이트, 즉, 대역폭(bandwidth)은 $\frac{16}{65} = 0.25$다.

이제 이를 개선하는 방법에 대해 가볍게 알아보도록 하자. 적당한 방법 중 하나는 메모리와 버스의 width를 넓히는 방법이고, 또 하나는 다수의 메모리 뱅크(memory bank)를 이용해 인터리빙(interleaving)하는 방법이다.

우선, 메모리와 버스의 width를 16 바이트로 넓히면 어떻게 될까? DRAM 액세스와 전송은 한 번만 발생하면 된다. 즉, $ 1 + 1 * 15 + 1 * 1 = 17$버스 사이클이다. 대역폭은 그에 따라 $0.94$가 된다.

4개의 메모리 뱅크에 의한 인터리빙을 적용한다면 어떻게 될까? 폭은 그대로이므로 데이터 전송 시간을 줄여주진 못하지만, 액세스는 한 번에 병렬적으로 일어나기 때문에 $ 1 + 1 * 15 + 1 * 4 = 20$버스 사이클을 요구한다. 대역폭은 $0.8$이다. 

인터리빙에 대한 내용은 나중에 따로 정리할 예정이다. 여기서 커버하는 내용은 아닌 듯해서...

### CPU Time
간단하게, CPU 타임을 다음과 같이 생각하자.

$ (CPUtime) = (CPUcc + MemoryStallcc) * (CC time) $

Memory stall cycle은 다음과 같이 나타낼 수 있다.

$ (MemStacllcc) = (TotalAccess) * (MissRate) * (MissPenalty) $

충분히 납득할 수 있는 식이다. 이제 예시를 통해 성능을 대략적으로나마 분석해보자.

> 명령어 캐시 미스(I-cache miss)의 비율은 2%, 데이터 캐시 미스(D-cache miss)의 비율은 4%다. 미스 패널티는 동일하게 100 사이클이고, base CPI, 즉, 이상적인 경우의 CPI는 2 사이클이다. 그리고 Load/Stroe는 전체 명령어의 36%만큼 실행되고, 읽기와 쓰기의 성능 차이는 고려하지 않는다.

I-캐시의 경우, 명령어 당 $0.02 * 100 =2$, D-캐시의 경우 $0.04 * 0.36 = 1.44$만큼의 미스 패널티가 있다. 달리 말하면, 미스 패널티를 모든 명령어가 똑같이 나눠가지면 각각 $2 + 1.44 = 3.44$ 사이클을 가지게 된다는 것이다. 즉, CPI는 $2 + (2 + 1.44) = 5.44$로, 이상적인 경우에 비해 2.72배 더 오래 걸린다.

CPI 말고도 캐시 성능을 확인하기 위한 지표가 있다. 바로 평균 메모리 접근 시간(AMAT, Average Memory Access Time)을 이용하는 것인데, 이것은 메모리 접근 연산 내에서만 고려한다. 즉, 메모리 접근 명령어의 비율 등을 따지지 않는다.

$ AMAT = (HitTime) + (MissRate) * (MissPenalaty) $

연습용으로 예시를 하나 생각해보자.

> CPU는 1ns의 클락을 가지고, 히트 타임은 1 사이클, 미스 패널티는 20 사이클이다. 또, 캐시 미스 비율은 5%다.

이 때의 AMAT은 $ 1 + 0.05 * 20 = 2 $사이클이고, 이는 2ns에 해당한다. 이러한 측면에서 보면, 히트 타임도 어느 정도 중요하다. 어떠한 메모리 구조는 미스 비율을 줄이지만 히트 타임을 증가시키는데(e.g. n-way associated), 이 둘 사이의 적절한 타협점을 찾아야 한다.

## Associative Caches
직접 매핑 캐시와 상반...까진 아니고 어느 정도 대립하는 캐시 구조다. 앞에서도 잠깐 언급했지만, 연관(associative) 되었다는 것은 일정 수의 블록이 집합을 이루어, 매핑되는 집합 내에선 어디에든 캐시될 수 있음을 의미한다. 위치를 결정하기 위한 블록 주소(혹은 블록 번호)는 직접 매핑과 유사한 방식으로 사용되지만, 이는 집합의 위치(혹은 번호)만을 결정하는 데 사용된다. 즉, 데이터가 들어갈 집합은 $(BlockNumber) mod (NumberofSet)$과 같다.

캐시 블록 수보다 작은 n에 대해, 연관 캐시는 n-way set associative와 fully associative, 두 종류로 나누어 볼 수 있다.

- n-way set associative : 각 집합은 n개의 엔트리(블록)를 가진다. 특정 집합에 매핑되는 블록은 n개의 위치에 존재할 수 있다.
- Fully associative : 캐시는 단일 집합으로 구성되어, 모든 블록은 캐시 내의 어디에든 위치할 수 있다.

![](/imgs/ca/ca6.png)

이러한 관점에서, 직접 매핑 캐시는 1-way associative한 캐시로 볼 수도 있다. 연관 정도에 따른 캐시 구조는 다음과 같이 나타내어진다.

![](/imgs/ca/ca7.png)

메모리 읽기를 시도할 때는, 주소에 의해 지정된 집합 내의 모든 엔트리를 찾는다. 연관 정도(associativity)가 클 수록 검색해야 하는 엔트리가 많아지는 셈이다.

![](/imgs/ca/ca8.png)

분명히 높은 연관 정도는 캐시 미스 비율을 낮춰주지만, 동시에 검색해야 하는 엔트리 수를 늘리기 때문에 히트 타임을 증가 시킨다. 다수의 comparator를 이용해 병렬적으로 검색하면 보다 빠르지만, 이는 또다른 비용을 낳는다. 또, 연관 정도를 두 배로 늘린다고 해서 미스 비율이 절반으로 줄어드는 것도 아니기 때문에, 용도에 따른 적절한 타협점을 찾아야 한다. VM 같은 경우는 무조건 미스 비율을 줄이는 게 좋기 때문에 완전 연관을 사용하는 것도 고려해 볼 수 있을 것이다. (VM 파트에서 다시 다루어보자)

## Replacement Policy
캐시 미스가 발생했을 때는 반드시 하위 계층으로부터 블록을 가져와야 한다. 직접 매핑 캐시의 경우 선택지가 없지만, 연관된 캐시라면 몇 가지의 선택지가 있다.

우선, 빈 엔트리, 즉 non-valid한 엔트리를 우선시한다. 만약 빈 엔트리가 없다면, 기존에 캐시되어 있던 엔트리 중 하나를 빼야 한다. 엔트리를 고르는 방법도 여러 가지가 있다.

- LRU(Least-Recently Used): 최근에 사용되었음을 알리는 상태 비트를 추가하여, 잘 사용되지 않은 오래된 엔트리를 우선적으로 교체한다. 이는 연관 정도가 클수록 구현하는 데 어려움이 있다.
- Random: 무작위로 교체시킬 블록을 결정한다. 높은 연관 정도에선 LRU와 어느 정도 비슷한 성능을 보여준다.

## Multilevel Caches
캐시의 사용에서 가장 치명적인 부분은 시간이 오래 걸리는 메인 메모리로의 접근이다. 메인 메모리로의 접근을 줄이기 위한 또다른 방법으로, 우리는 여러 계층의 캐시를 사용할 수 있다. 즉, 2차, 3차 캐시를 도입하여 원래의 1차(primary) 캐시와 메인 메모리 사이에 놓는 것이다. 물론, 이들은 1차 캐시보다 느리지만, 그래도 메인 메모리보다는 훨씬 빠르고 용량도 크다. 

추가적인 캐시를 사용하는 경우, 1차 캐시에서 미스가 발생해도, 이후의 캐시에서 히트가 발생하면 메인 메모리로의 접근은 하지 않기 때문에 비용이 적게 든다. 물론, 최종 캐시에서 미스가 발생했을 땐, 더 많은 과정을 거쳤기에 비용이 조금 더 많이 든다. 하지만 그럼에도 다단계 캐시의 사용은 꽤 효과적이다. 다음 예시를 통해 성능을 비교해보자.

> Base CPI가 1 사이클이고 clock rate가 4GHz(사이클 당 0.25ns)인 환경을 가정한다. 명령어 당 미스 비율은 2%, 메인 메모리 접근 시간은 100ns다.

단일 캐시에서는, 미스 패널티는 $\frac{100}{0.25} = 400$사이클이고, 유효 CPI는 $1 + 0.02 * 400 = 9$사이클이다. 이제 다음과 같은 2차, L-2 캐시를 생각해보자.

> 2차 캐시로의 접근 시간은 5ns다. Global miss rate, 즉, 모든 단계에서 캐시 미스일 확률은 0.5%다. 

이러한 가정은 정성적으론 꽤 납득할 만하다. 2차 캐시가 충분히 크다면 메인 메모리에서의 미스 비율을 낮춰줄 것이기 때문이다. 정량적인 디테일은 잘 모르겠다. 책의 예시를 그대로 가져왔으니 난 Patterson씨를 믿는다.

아무튼, 여기서 1차 캐시만 미스가 발생할 경우 미스 패널티는 $ 5/0.25 = 20$사이클이다. 2차 캐시까지 미스가 발생할 경우엔 2차에서 메인 메모리에 접근한 뒤, 이를 가져와 1차가 2차로의 캐시 읽기를 재시도하도록 해야 한다. 앞에서의 400 사이클보다 더 많이 비용이 발생하는 셈인데, 대충 500 사이클이라 하자. 그렇다면, CPI는 $ 1 + 0.02 * 20 + 0.005 * 500 = 3.9$사이클이다. 1차 캐시만 사용하는 경우보다 훨씬 비용이 적게 든다.

다단계 캐시를 사용할 땐 고려해야 할 점이 있다. 1차 캐시는 히트 타임을 줄이는 걸 목표로 하는 게 좋다. 미스 시에도 패널티가 상대적으로 적고, 히트 타임을 줄여 프로그램의 전반적인 클럭 사이클 단축과 파이프라이닝의 축소를 도모하는 게 이득이기 때문이다. 반면 2차 캐시는 미스 패널티가 굉장히 크므로 미스 비율을 줄이는 방향으로 설계되는 게 좋다.

이러한 사항을 고려하여 다단계 캐시를 구현하면, 단일 캐시에 비해 1차 캐시는 사이즈가 작고, 블록 크기도 작아진다. 2차 캐시는 보다 연관되어 있고, 그 크기 자체도 커지게 된다.

## Algorithms for Cache
캐시 성능의 향상을 위해선, 프로그램도 캐시 성능을 고려하여 설계되어야 한다. 캐시는 지역성을 기반으로 설계되었기 때문에, 좋은 지역성을 가지면 좋은 캐시 성능을 보일 거란 점은 앞에서 언급했다. 

또, 블록화이란 방법이 있다. 어떤 배열이나 행렬 등을 처리할 때, 전체를 처리하기보단 부분배열, 부분행렬을 따와 블록화하여 이를 블록 단위로 처리하는 알고리즘이다. 이는 캐시된 블록이 버려지기 전에 최대한 사용함으로써 미스 비율을 낮추고 시간적 지역성을 확보하는 방법이다.

물론, 대부분의 경우 똑똑한 컴파일러가 좋은 캐시 성능을 이끌어내기 위해 최적화를 해줄 것이다. 다만 optimization blocker가 없는지 확인을 잘 해주도록 하자.

## CPUs for Cache
명령어는 순차적으로 처리될 것으로 보이지만, 그렇지 않을 수도 있다. 이는 분기나 점프같은 경우를 말하는 게 아니라, 여러 명령어들의 순서가 뒤바뀔 수 있음을 의미한다. 의존성(dependancy)이 없는 명령어들이라면 그렇게 해도 괜찮을 것으로 보인다. 이러한 방식을 비순차적 명령어 처리(out-of-order execution)라고 한다.

이런 비순차적 처리를 지원하는 CPU는 비블로킹 캐시(nonblocking cache) 구조를 사용하여, 캐시 미스에 의한 패널티를 감출 수 있다. 캐시 미스 발생 시 다른 작업을 통해 패널티를 어느 정도 줄인 것처럼 보이게 하거나, 여러 캐시 미스를 중첩시켜 총 패널티가 줄어든 것처럼 보이게 할 수 있다. 물론 이러한 방식은 병렬적인 처리가 가능한 설계 및 충분한 대역폭을 요구한다. 이 내용은 나중에 다시 짚어보도록 하자.

# Other Issues
## 3C: Sources of Misses

## Cache Coherence Problem


# 마치며
우리는 지금까지 캐시의 구조와 동작 방식을 알아보면서도, 어떻게 해야 좋은 성능을 이끌어낼 수 있을지에 대한 고민을 계속 해왔다. 연관이나 교체나 쓰기 방식이나, 모두 다 정확한 동작을 유지하면서도 좋은 성능을 이끌어내기 위한 노력의 일환이다. 컴퓨터 구조에서 성능 문제는 절대로 빼놓을 수 없는 셈이다.

아무튼 다음엔 가상 메모리(VM)와 메모리 계층 구조에 관한 나머지 주제들을 알아보려고 한다.

생각보다 포스트가 굉장히 길었다.
