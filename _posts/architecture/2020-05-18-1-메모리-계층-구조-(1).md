---
title: 1 메모리 계층 구조 (1)
tags: Computer-Architecture
toc: true
---

# Basic Principle
많은 CSE 과목에서 메모리 계층 구조(memory hierarchy)에 대한 개념을 한 번은 꼭 짚고 넘어간다. 지금도 한 세 번째는 우려먹는 것 같다. 아무튼 그래서 전반적인 내용은 알고 있다고 가정하고 정리하려 한다.

컴퓨터 내엔 여러 종류의 메모리가 있고, 이들은 용량, 접근 속도, 용도 등의 측면에서 서로 다르다. 예를 들어 SRAM은 빠르지만 용량 대비 굉장히 비싸고, 디스크는 가격 대비 매우 큰 용량을 지니고 있지만 무척 느리다. 또, 빠른 메모리일 수록 CPU와 (개념적으로) 가까이 있다.

![](/imgs/ca/ca2.png)

즉, 속도와 비용은 동시에 만족하기가 굉장히 어렵다. 이러한 한계를 극복하기 위한 시도 중 하나가 메모리 계층 구조를 활용하는 것이다.


# Locality
메모리 계층 구조를 논할 때, 지역성(locality)는 항상 빠지지 않는다. 왜냐? 메모리 계층 구조는 좋은 지역성을 가진 프로그램을 가정하고, 그에 맞추어 성능적인 측면에서의 이점을 가져가기 때문이다. 달리 말하면, 나쁜 지역성을 가진 프로그램의 경우, 좋은 지역성의 프로그램보다 나쁜 퍼포먼스를 보일 것이다.

즉, 우리는 메모리 계층 구조를 설계하고 이해할 때, 그것이 지역성을 잘 활용하여 좋은 성능을 이끌어내는지 확인해야 한다. 프로그램을 설계할 때도, (우리가 사용하는 컴퓨터는 지역성을 충분히 활용함을 알고 있으므로) 그것이 좋은 지역성을 가지는지 확인해야 한다.

지역성은 두 종류로 나눌 수 있다.

- 시간적 지역성(temporal locality): 최근에 액세스한 대상은 이후에 또 다시 액세스할 가능성이 크다.
- 공간적 지역성(spatial locality): 최근에 액세스한 대상 근처의 대상도 액세스될 가능성이 크다.

시간적 지역성의 대표적인 예시는 for loop의 `i` 등이 있고, 공간적 지역성의 대표적인 예시는 순차적인 명령어 접근이나 배열(array) 등이 있을 것이다.

    for (int i=0; i < n; i++){
   	    for (int j=0; j < n; j++){
    		Arr[j][i] = 0;
   		}	
    }

위와 같은 코드는 공간적 지역성이 잘 지켜지지 않은 케이스다. 배열은 row-major하게 할당되는데, 저러한 방식은 주소를 계속 건너뛰면서 참조하기 때문이다. 


# Cache Memory
보통 캐시(cache)는 상대적인 개념으로, 낮은 계층의 데이터를 가져와 접근 속도를 높이는 방법 혹은 그러한 역할을 수행하는 계층을 의미한다. 캐시 메모리(cache memory)는 CPU와 가장 가까이 있어, 메인 메모리로부터 캐시 역할을 하는 메모리를 의미한다.

그 전에, 용어를 간단히 정리하고 가자.

## Terminologies
블록(block), 또는 라인(line)은 캐시에서의 복사본의 기본 단위다. 각각의 블록이 가지는 크기는 아키텍처마다 다를 수 있다. 하나의 워드만 포함할 수도, 몇 개의 워드를 포함할 수도 있다.

![](/imgs/ca/ca1.png)

우리는 낮은 계층의 메모리가 가진 데이터에 접근하려 할 때, 직접 그곳에 접근하기보단 캐시를 활용한다. 하지만 모든 데이터가 캐시 내에 있지는 않다. 찾던 데이터가 캐시에 있는 상황을 hit라 하고, 그렇지 않은 상황을 miss라 한다.

캐시 미스가 발생했을 때, 캐시는 낮은 계층으로부터 해당 데이터를 가져와야 한다. 이 때 걸리는 시간을 miss panelty라 하며, 캐시 미스가 발생하는 비율을 miss rate라 한다.

## Mapping
캐시는 하위 계층으로부터 데이터를 가져와 어떻게 저장할까? 또, 해당 데이터를 어떻게 찾을 수 있을까? 크게 세 방법으로 나눌 수 있다.

- Direct-mapped : 하위 계층의 주소를 이용해 캐시 내 주소를 유일하게 결정한다. (1-way associative)
- n-way associative : 캐시를 n개의 블록을 가지는 집합으로 분할하여, 하위 계층의 주소를 이용해 캐시 내 집합을 유일하게 결정한다.
- fully associative : 블록은 캐시 내 어디든지 위치할 수 있다.

각각은 장단점을 지니고 있다. 직접 매핑(direct-mapped) 캐시는 블록을 찾는 데 비용이 거의 들지 않지만, fully associative하다면 비용이 상당히 발생할 것이다. 반대로, 직접 매핑 캐시에서 같은 위치에 캐시되는 두 블록이 번갈아가며 참조되는 경우를 생각해볼 수 있을 것이다. 블록을 교체하는 과정이 계속 반복되는 셈이다. 연관된(associative) 캐시는 그러한 경쟁이 적기 때문에 이러한 점에선 효과적이다.

각각의 방법을 하나하나 알아볼 예정이다. 또, 직접 매핑 방식에서 캐시에 대한 전반적인 동작 과정을 알아보도록 하자.

## Direct-Mapped Cache & Cache Basics
캐시가 어떻게 작동하는지 알아보자. 이를 위해 우선 직접 매핑 캐시를 먼저 알아보고, 이를 기반으로 캐시의 동작 과정을 살펴볼 것이다.

직접 매핑 캐시에서는, 주어진 메모리 블록 주소에 의해 캐시 내 위치가 유일하게 결정된다. 즉, 다른 선택지가 없다. 이는 장점이자 단점이 될 수 있는데, 선택지가 없기 때문에 이후 캐시에서 블록을 찾을 때도 별도의 탐색 없이 바로 블록을 찾을 수 있다. 하지만 공간이 굉장히 많이 남아있음에도 블록을 교체해야 하는 상황이 발생할 수 있다. 블록의 교체는 하위 계층 메모리의 접근을 수반하므로, 비용이 꽤 크게 발생한다.

![](/imgs/ca/ca3.png)

위의 그림을 자세히 보면, 캐시 내 블록 주소는 메모리 블록 주소의 하위 3비트와 같음을 확인할 수 있다. 이는 우연일까? 물론 그렇지 않다. 대부분의 직접 매핑 캐시는 다음과 같은 방식으로 캐시 내 주소를 결정한다.

$$Addr = (Block address) modulo (blocks in cache, 2^n)$$

여기서 캐시의 블록 수는 $2^3$이므로, 주어진 블록 주소에 대해 하위 3비트를 뜯어내는 것이다. 

당연하게도, 한 캐시 블록엔 여러 메모리 블록이 들어올 수 있다. 그러면 해당 블록이 우리가 찾는 블록이 맞는지 어떻게 확인할까? `Tag`와 `Valid`를 사용함으로써 해결할 수 있다. 태그(tag)는 주소 결정에 사용되지 않은 비트(`Index`)를 제외한 상위 비트로, 인덱스와 태그를 사용하면 메모리 주소가 하나로 결정된다. 또, 해당 영역이 비어있는지 아닌지의 확인도 필요하다. 유효(valid) 비트를 사용하여 캐시에 데이터가 있는지(1) 아닌지(0) 표시 및 확인할 수 있다.

### Cache Example
위의 그림으로부터, 메모리 주소가 `10101`인 블록을 캐시한다고 생각해보자. 캐시 블록 수가 8이므로, 인덱스는 하위 3비트인 `101`이 되고, 태그는 나머지 상위 비트인 `10`이 된다. 표로 나타내면 다음과 같다.

Index | Valid | Tag | Data
---|---|---|---
... | ... | ... | ...
100 | 0 | | 
101 | 1 | 10 | Mem\[10101]
... | ... | ... | ...

여기서 메모리 주소가 `10100`인 블록을 참조하려고 하면? Valid가 0이기 때문에 캐시 미스를 발생시키고, 하위 메모리로부터 데이터를 가져올 것이다. 즉, 블록 `10100`을 캐시하게 된다.

Index | Valid | Tag | Data
---|---|---|---
... | ... | ... | ...
100 | 1 | 10 | Mem\[10100] 
101 | 1 | 10 | Mem\[10101]
... | ... | ... | ...

이제 메모리 주소가 `11101`인 블록을 읽어보자. 캐시에서 `101` 위치는 valid하지만 태그가 일치하지 않으므로 이 또한 캐시 미스를 발생시키고, 블록 `11101`로 해당 영역을 교체할 것이다.

Index | Valid | Tag | Data
---|---|---|---
... | ... | ... | ...
100 | 1 | 10 | Mem\[10100] 
101 | 1 | 11 | Mem\[11101]
... | ... | ... | ...

### Address Subdivision
우리는 주어진 메모리 주소를 상위의 태그와 하위의 인덱스로 나누었었다. 조금 더 자세히 주소를 분할 방식을 알아보자. 

![](/imgs/ca/ca4.png)

인덱스를 위한 비트 수($n$)는 캐시 블록의 수($2^n$)에 대응된다. 나머지 상위 비트는 태그 비트가 된다. Byte offset은 주어진 블록에서의 변위를 지정하기 위해 쓰인다. 블록 내에서 0-3 바이트를 이동할 수 있으니, 달리 말하면 각각의 블록은 4바이트(1워드)의 크기를 가지고 있음을 알 수 있다. 주소 분할 방식만 봐도 캐시 구조의 일면을 볼 수 있는 셈이다. 연습 겸 예시를 하나 보자.

> 각 블록은 16바이트 크기고, 캐시는 64개의 블록을 가진다. 32비트 주소를 사용한다고 가정했을 때, 메모리 바이트 주소가 1200인 블록은 어디에, 어떻게 저장될까?

우선 주소를 적절하게 분할해보자. $2^4 = 16$바이트 크기이므로 바이트 오프셋은 총 4비트가 필요하다. 캐시 블록 수는 $2^6 = 64$개이므로, 인덱스엔 6비트를 할당해주자. 나머지 22비트는 태그 비트가 된다.

![](/imgs/ca/ca5.png)

바이트 주소가 1200이면, 블록 주소는 $\lfloor\frac{addr_byte}{block_size}\rfloor = \lfloor\frac{1200}{16}\rfloor =75$다. 캐시 내 블록 주소는 $75 mod 64 = 11$이다. 즉, 001011이 인덱스로, 00...001이 태그로 들어간다.

### Block Size Considerations
블록 사이즈는 어떻게 정해야할까? 답은 '적당히'다.

큰 블록 사이즈는 공간적 지역성에 의해 캐시 미스의 빈도를 줄여준다. 그 이유를 예시 하나를 통해 생각해보자. 어떤 적당히 큰 배열 A가 있고, 배열의 원소 4개의 크기만큼을 블록 사이즈로 정했다고 가정한다. 그 배열에 대해 for-loop 등으로 순차적인 탐색을 한다면, 아마 원소 4개를 탐색할 때마다 배열 원소 일부를 캐시할 것이다. 최소 3번은 캐시 히트를 보장하는 셈이다. 만약 블록 크기가 원소 8개의 크기라면? 7번은 캐시 히트를 보장해줄 것이다.

하지만 여기엔 문제가 있는데, 증가된 블록 사이즈만큼 캐시 내의 블록 수는 줄어든다. 이는 같은 위치에 캐시될 블록 간 경쟁을 증가시켜 어느 시점부턴 오히려 캐시 미스 빈도(miss rate)가 증가하게 된다. 또, 큰 블록 사이즈는 과도한 prefetching으로 인한 pollution을 발생시킨다. 위의 예시에서, `A[0]`만을 사용하려고 했는데 `A[0]`부터 `A[7]`까지 모두 캐시하는 것은 분명한 낭비다. 모든 원소를 쓴다는 보장도 없고.

또, 많은 양의 캐시는 그 자체로 더 오래 걸린다. 큰 사이즈의 블록으로 얻어낸 이득을 여기서 잃을 수 있는 셈이다. 하지만 이는 해결할 방법이 몇 있다. 블록 내에서 필요한 영역부터 우선적으로 전송하는 방법(critical-word-first)과 블록 전체를 기다리지 않고 요청된 영역 도착 시 바로 실행하는 방법(early restart) 등이 있다. 

### Cache Miss
캐시 히트(hit) 시엔 별 문제가 없지만, 캐시 미스(miss)가 발생하면 메모리로부터 요청 데이터를 가져오고 다시 요청토록 해야 한다. 이를 위해 CPU 파이프라이닝을 어느 정도 지연시켜야 하고(메모리 읽기는 상대적으로 오래 걸린다), 하위 계층으로부터 데이터를 읽어야 하며, 이후 캐시 미스가 발생한 명령을 재시작해야 한다.

### Writing
메모리로의 쓰기는 크게 두 가지 방법이 있다. Write-through와 Write-back인데, 하나하나 알아보자.

#### Write-Through
Write-through는 쓰기 수행 시 캐시와 그에 대응되는 메모리 양 쪽에 쓰는 방식이다. 다만 메모리로의 쓰기는 굉장히 느리기 때문에, 이러한 방식은 저장(store)을 위한 명령어의 사이클을 엄청나게 증가시킨다.

이는 쓰기 버퍼(write buffer)를 이용해 어느 정도 완화할 수 있다. 쓸 내용을 버퍼에 저장하고 프로세서는 하던 일을 계속 한다. 그동안 버퍼에 있는 내용을 메모리에 쓰면 된다. 물론 이는 버퍼가 계속 꽉 차는 상황이 발생하면 난감해진다. 연속적으로 쓰기를 수행하면 지연이 발생할 것이다. 또, 만약 쓰기를 처리하는 속도가 버퍼보다 빠르다면 안쓰는 것만 못하다.

#### Write-Back
Write-back은 쓰기 수행 시 캐시에만 쓰고, 이후에 해당 블록이 다른 블록으로 교체될 때만 메모리에 쓰기를 수행하는 방식이다. 즉, 한 블록에 대해 여러 번 쓰기를 수행해도 메모리에는 한 번만 쓰면 되는 셈이다.

우리는 캐시에 `Dirty` 비트를 추가함으로써 이를 구현할 수 있다. 각 블록에 쓰기가 수행되어 교체 시 해당 내용을 메모리에 써야 함을 이 비트로 알린다. 만약 비트가 0이라면 별 작업 없이 블록 내용을 밀어버려도 괜찮다.

#### Write Allocation
앞서 말했듯, 쓰기 수행을 위해선, 우선 블록이 캐시 내에 있는지부터 확인해야 한다. 히트된다면 별 문제 없지만 미스 시 생각해봐야 할 문제가 있다. 

Write-through에서 캐시 미스 시엔 어떻게 해야 할까? 블록을 가져오는 방법(write allocation)과 블록을 가져오지 않고 메모리만 갱신하는 방법(write around)이 있다. 전자는 충분히 납득할 만 하지만, 후자는 어떻게 보면 굉장히 비효율적으로 보인다. 쓰고 나서 이후의 읽기에서 캐시 미스를 발생시키기 때문이다. 하지만 데이터의 초기화(initialization)같은 상황에서는 이 방법이 괜찮을 수도 있다. 프로그램 시작 부분에서 변수 초기화가 몰려있다면, 전자의 방식은 분명한 캐시의 낭비기 때문이다.

Write-back의 경우, 캐시 미스를 확인했을 때 교체해야 하는 영역이 dirty하다면(갱신해야 한다면), 원래 블록을 메모리에 쓴 뒤 교체해주어야 한다. 그렇지 않으면 원래 블록을 날려먹을 수 있기 때문이다. 다만 그렇게 되면 예전 블록을 쓰기 위한 시간만큼 지연이 발생하는데, 우리는 쓸 내용을 버퍼에 저장하고 실행을 이어감으로써 이를 해결할 수 있다.

## Cache Performance
이제 캐시가 어떻게 동작하는지는 대충 알아본 것 같다. 이제 성능에 관한 이야기를 해보자. 우선 다음과 같은 상황을 가정한다.

> DRAM이 메인 메모리로 사용되고, 이는 고정 width에 고정 주기의 버스 클락(bus clock)을 가진 버스에 연결되어 있다. 주소 전달에 1 버스 사이클이, DRAM 액세스에 15 버스 사이클이, 데이터 전송에 1 버스 사이클이 걸린다. 블록은 16 바이트 크기를 가지며, DRAM은 4 바이트의 width를 가진다. 

이러한 상황에서 캐시 미스에 의한 패널티는 얼마나 클까? 다음과 같은 방식으로 구할 수 있을 것이다.

$$ (Miss Penalty) = (Addr_Transfer) + (Access) * \frac{(Words)}{(Word-Wide)} + (Data_Transfer) * (words) $$







