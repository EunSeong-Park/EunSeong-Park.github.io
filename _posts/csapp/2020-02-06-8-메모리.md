---
title:  "[CS:APP 8] 메모리"
toc: true
tags: CS:APP
---

# Intro
이번 단원에선 저장장치의 종류나 그들의 계층, 그리고 캐시에 대해 배울 예정이다. 내용이 조금 많은데, 최대한 정리하고 요약해서 써야겠다.


# 메모리의 종류
## RAM (Random Access Memory)
Volatile memory의 일종으로, 전원 공급이 끊기면 정보가 손실되는 특성을 지닌다. SRAM(Static-RAM)과 DRAM(Dynamic-RAM)으로 분류 가능한데, SRAM이 더 비싸고, 빠르며, 용량이 적다. 구체적인 구조와 동작 원리는 짚고 넘어갈 필요가 없어보인다. 궁금할 땐 책을 읽어보도록 하자.

![](/imgs/csapp/18.png)

## ROM (Read Only Memory)
대표적인 nonvolatile memory 중 하나다. Read-only라고 단어에 명시되어있긴 하나, 실제론 쓰기가 가능한 ROM이 대다수지만, 역사적 맥락에서 그 이름을 유지하고 있다고 한다. ROM은 그것의 reprogram 방식에 따라 분류된다.

ROM | 재기록 방식
---|---
ROM (혹은 Mask ROM) | 생산 시 정보가 기록되어 reprogram이 불가능하다.
PROM (Programmable ROM) | 단 한 번 사용자가 정보를 기록할 수 있다. 
EPROM (Eraseable PROM) | 광학적인 성질에 의해 반복적으로 정보를 지울 수 있다.
EEPROM (Electrically Ereaseable PROM) | PCB(회로 판)에서 물리적 장치 없이 반복적으로 정보를 지울 수 있다.

## Disk
RAM이 (특히 SRAM) 적은 용량과 빠른 속도를 특징으로 한다면, 디스크는 그 반대로 느리지만 큰 용량을 특징으로 한다고 볼 수 있다. 책에선 디스크의 구조와 그에 따른 용량 계산, 액세스에 걸리는 시간 등이 자세히 나와있고 예제도 많지만, 딱히 중요하다고 여겨지진 않으므로 생략.


# Memory Access
프로세서는 메모리와의 지속적인 데이터의 교환이 필요하다. 앞에서 Assembly로부터 알 수 있듯, 데이터는 특정 장치 내에서만 갇히지 않고 레지스터와 메모리 사이를 오간다. 이제 프로세서가 각 메모리(메인 메모리 or 디스크)와 어떻게 정보를 교환하는지 알아보려 한다.

기본적으로, 메모리와 프로세서 사이엔 버스(bus)라는 주소, 데이터, 제어 신호 등을 포함하는 병렬 선들의 집합으로 이루어져 있다. 버스는 여러 장치들에 의해 공유될 수 있으며, 각각의 전송(transaction이라 하자.)은 데이터가 보내지는 것뿐 아니라, 그것의 종류, 전송 경로, R/W 여부 등이 제어 신호로 전달될 수 있다. 

## Main Memory Access

![](/imgs/csapp/19.png)

메인 메모리(DRAM)는 위와 같은 구조로 프로세서와 연결되어 있다. 프로세서는 어떻게 메모리로부터 정보를 가져오고, 처리한 정보를 어떻게 메모리에 저장할까? Assembly로 생각해보면, 후자의 경우 `movq Address, %Register`로, 후자의 경우 `movq %Register, Address` 정도로 볼 수 있겠다. 

### Memory Read
1. CPU가 주소를 memory bus에 보낸다.
2. 메인 메모리는 bus로부터 주소를 읽고, 해당 주소에 저장된 데이터를 찾아 bus에 전송한다.
3. CPU는 bus로부터 데이터를 읽은 뒤, 이를 레지스터에 복사한다.

### Memory Write
1. CPU가 주소를 memory bus에 보낸다.
2. 메인 메모리는 해당 주소를 받고 데이터를 기다린다.
3. CPU는 데이터를 memory bus에 보낸다.
4. 메인 메모리는 bus로부터 데이터를 읽고, 이를 해당 주소에 저장한다.

## Disk Access

![](/imgs/csapp/20.png)

일반적인 컴퓨터엔 많은 종류의 I/O(입출력) 장치들이 있다. I/O bus는 그들 사이에서 정보를 전달하는데, 디스크 또한 이 I/O bus를 통해 읽고 쓰인다. 어떻게 디스크 내 정보가 읽어지는지 간단히 알아보자.

1. CPU는 명령어, 논리블록 번호(logical block number), 메모리 주소를 포트에 쓴다.
2. 디스크 컨트롤러는 해당하는 디스크 섹터를 읽은 뒤 DMA(Direct Memory Access)를 통해 메인 메모리로 전송한다.
3. 전송 후, 디스크 컨트롤러는 CPU에게 interrupt를 걸어 알려준다.


# Locality
Locality(지역성)이라는 개념이 있다. 지역성을 잘 만족시킬 수록, 더욱 효율적이고 좋은 성능을 지닌 '잘 만든' 프로그램이 될 수 있다. 시스템의 많은 부분은 지역성을 충분히 활용할 수 있게 설계되었기 때문에, 우리는 프로그램이 좋은 지역성을 가질 수 있도록 노력해야 한다. 그래야 빨라지니.

- Tempral Locality : 최근 참조된 메모리 위치는 가까운 미래에 다시 참조될 가능성이 높다.
- Spatial Locality : 어떤 메모리 위치가 참조되었을 때, 가까운 미래에  그 근처의 메모리 위치도 참조될 가능성이 높다.

## Locality의 예시

    sum = 0; 
    for (i = 0; i < n; i++) 
    sum += a[i]; 
    return sum; 
    
위 코드를 보자. 

- 매 반복 마다 `sum`과 counter 역할을 하는 `i`가 반복적으로 참조되고 있다. (Temporal Locality)
- 매 반복 마다 array, a는 바로 옆 위치의 원소가 참조되고 있다. (Spatial Locality)

이에 더하여, 각 instruction의 주소가 sequential하게 배열되어 있는 것도, 루프로 인해 동일한 instruction이 반복적으로 실행되는 것도 locality의 일종으로 볼 수 있을 것이다.

## Locality의 정성적 측정
지역성은 정량적으로 측정되기엔 조금 어려운 감이 있지만, 주어진 코드의 지역성이 좋은지 나쁜지 정도는 어느 정도 판단할 수 있을 것이다. 특히, 명백하게 나쁜 지역성의 코드라면 더더욱 그럴 것이다.

    int sum_array_3d(int a[M][N][N])
    { 
        int i, j, k, sum = 0;
        for (i = 0; i < M; i++)
            for (j = 0; j < N; j++)
                for (k = 0; k < N; k++)
                    sum += a[k][i][j];
        return sum; 
    } 

가령 위의 코드는 굉장히 나쁜 spatial locality를 가지고 있다. 매 반복마다 sum에 더할 값의 주소가 앞뒤로 점프해야 한다. 이는 array가 row-major로 각 원소가 배열된다는 사실에 기반한다. Array의 구조를 모르면 locality가 해쳐지는지도 모르는 셈이다. 달리 말하면, 데이터 구조나 메모리, 혹은 시스템에 대한 이해가 있어야 locality를 지켜낼 수 있다고 볼 수 있겠다.


# Memory Hierarchy
컴퓨터 내에는 속도, 용량, 역할 등의 측면에서 다양한 종류의 메모리가 있어, 그들은 각자의 역할을 수행하면서도 서로 데이터를 전달하는 등의 상호작용을 할 수 있다. 메모리들은 공통적으로 하나의 경향을 가지는데, 바로 비쌀 수록 빠르다는 점이다. 레지스터는 굉장히 빠르지만 용량이 적고 비싸다. 디스크는 굉장히 느리지만 (레지스터, RAM 등에 비해) 굉장히 많은 용량을 지니고 있다. 이를 통해 우리는 메모리들을 계층화하여 나타낼 수 있는데, 이를 __Memory Hierarchy (메모리 계층 구조)__ 라 한다. 

![](/imgs/csapp/21.png)


# Cache
캐시(cache)란 어떤 장치에 저장된 데이터를 위한 준비 영역으로 사용하는, 그것보다 더 빠르고 적은 용량(상위 계층)의 장치를 의미한다. 달리 말하면, k 계층의 장치는 k+1 계층의 캐시 역할을 한다고 볼 수 있다. 그리고 이러한 캐시를 사용하는 과정을 캐싱(caching)이라 부른다.

그렇다면, 캐시는 무슨 역할을 하고, 컴퓨터와 우리에게 어떤 이익을 줄까?

## 캐시의 동작 원리
우리가 k+1 계층에 저장된 데이터를 참조하려 할 때, 만약 해당 데이터가 k 계층에, 즉 캐시에 저장되어 있다면 속도가 느린 k+1 계층을 거치지 않고 바로 데이터를 가져올 수 있다. 즉, 속도가 빨라지는 셈이다. 하지만 메모리 계층 구조에 의하면, 상위 계층의 장치는 용량이 적기 때문에 하위 계층의 데이터를 전부 가져올 수 없다. 즉, 가장 참조할 가능성이 높은 하위 계층의 데이터의 subset만을 따와야 하는 것이다. 그리고 그러한 subset은 바로 __지역성__ 에 근거하여 찾는다. 우리가 지역성이 좋은 프로그램을 만들어야 하는 이유다.

![](/imgs/csapp/22.png)

캐시에 타겟 데이터가 있어 빠르게 가져올 수도 있지만, 아무리 좋은 지역성을 가졌더라도 캐시에 타겟 데이터가 없는 경우가 있을 수 있다. 가능한 상황들에 대해 간단히 알아보자.

![](/imgs/csapp/23.png)

위의 상황처럼 요청받은 데이터가 캐시에 존재하는 경우를 cache hit(캐시 적중)라고 한다. 이런 경우, 정상적으로 데이터를 건네줄 수 있고, 별 문제가 되지 않는다.

![](/imgs/csapp/24.png)

그러나 이처럼 요청받은 데이터가 캐시에 없을 수도 있다. 이를 cache miss라 하는데, 그러면 캐시는 하위 계층 메모리로부터 요청받은 데이터를 요구하고, 해당 데이터를 캐시로 가져온다. 또, 캐시가 가득 찼을 경우, 캐시 내의 어떤 부분을 지우고 대체하게 된다. 결국, 캐시 미스가 발생해도 이후엔 요청받은 데이터가 캐시에 남아있게 되는 셈인데, 이는 temporal locality와 큰 관련이 있다. 다음에 같은 데이터를 또 요청받으면 그 땐 캐시 히트가 될 가능성이 높아지고, 그에 따라 수행 속도가 빨라질 수 있기 때문이다.

미스 발생 시마다 특정 policy에 맞추어 타겟 데이터를 캐시에 저장해야 한다. 즉, 데이터가 어느 곳에 저장될지, 대체해야 한다면 어떤 데이터를 victim으로 설정해야 한다는 말이다. 만약 데이터가 무작위로 배치된다면, 그것을 찾는데 너무 큰 cost가 발생한다. 해싱과 유사하게 mod 연산자를 이용하는 방법도 있고, 다른 많은 방법이 있겠지만 별로 중요한 것 같지는 않다.

## 캐시 미스의 종류
1. cold (compulsory) miss : 캐시가 비어있을 때 발생하는 캐시 미스. 시스템, 혹은 프로그램이 처음 시작된다면, 프리 페치를 하지 않는 이상 반드시 발생한다.
2. conflict miss : 어떤 둘 이상의 데이터가 같은 캐시 메모리 주소에 할당되어 발생하는 캐시 미스. 예를 들어, 특정 캐시 메모리 위치에 A, B가 할당될 수 있는데, A, B, A, B, ... 과 같은 식으로 요청하면 반복적으로 미스가 발생되어 성능이 저하될 수 있다. 이는 캐시 공간과 무관하다.
3. capacity miss : 캐시 공간이 부족해서 발생하는 캐시 미스. 

## 캐싱의 종류

![](/imgs/csapp/25.png)


# Cache Memory
캐시 메모리는 굉장히 빠르고 작은 SRAM 베이스의 메모리다. 

![](/imgs/csapp/26.png)

## 캐시 메모리의 구조
캐시 메모리는 S 개의 set을 가지며, 각 set은 E 개의 line을 가지고, 각 line은 B byte의 데이터 및 태그를 비롯한 각종 비트를 가지고 있다. 캐시의 사이즈, C는 전체 데이터 바이트의 수, 즉, C = S x E x B로 나타낼 수 있다.

![](/imgs/csapp/27.png)

이러한 구조 하에서, 만약 CPU로부터 특정 메인 메모리 주소에서 특정 길이의 word를 읽으라는 명령이 온다면, 캐시는 해당 word의 copy가 있는지 확인 후 (만약 있다면) 해당 word를 CPU에 보낸다. 그렇다면 캐시는 어떻게 copy의 존재 여부를 (빠르게) 판단할 수 있을까? set 하나 당 하나의 line을 가지는 경우(E = 1)에서의 동작 방식을 보며 캐시 메모리의 동작 방식을 간단히 살펴보자.

## 캐시 메모리 읽기
가장 간단한 형태의 캐시로, set 하나에 line 하나가 있는 캐시 구조다. 

![](/imgs/csapp/28.png)

![](/imgs/csapp/29.png)

특정 메모리 주소의 워드를 읽기 위한 비트(w)는 tag bit(with t bits), set index bit(with s bits), block offset(with b bits)로 분할될 수 있다. 그 중 set index bit(중간 비트)는 set을 선택하기 위한 index와 매핑되고, block offset(하위 비트)은 line 내의 bit index에 매핑된다. 그렇게 찾은 위치에서, 메모리 주소의 tag bit(상위 비트)와 캐시 라인 내의 tag bit와 일치하면 캐시 히트가 된다. 만약 tag bit의 불일치로, 혹은 0의 valid bit를 만나 캐시 미스가 일어나면 그 라인에서 메모리를 캐시한다.

set 당 line 수(E)가 2 이상인 set associative cache와 캐시 쓰기는 다루지 않는다.


# 캐시 친화적 코드
결국 캐시는 지역성에 기반한 매커니즘으로 빠른 메모리 접근을 가능케 해주므로, 좋은 지역성을 가져 최소한의 캐시 미스를 발생시키는 cache-friendly한 코드를 짤 수 있어야 한다. 그렇다면 어떻게 해야 캐시 친화적인 코드를 만들 수 있을까?

1. 가장 많이 사용되고, 내부에서 많은 계산 및 메모리 액세스가 일어나는 공통적인 케이스(핵심 함수 또는 루프)를 빠르게 만든다.
2. 루프나 함수 내부에서의 캐시 미스를 최소화한다. 예를 들어, 로컬 변수의 반복적인 참조는 컴파일러가 이를 레지스터에 캐싱할 수 있게 해 temporal locality를 향상시켜주고, stride-1 패턴의 참조는 모든 계층의 메모리에서 데이터를 연속적으로 저장할 수 있게 해 spatial locality를 향상시켜준다.



상당히 길게 썼는데, 그다지 만족스럽게 쓰진 못한 것 같다. 그래도 이해를 못하고 받아적은 부분은 없어 다행이다. 다음부턴 링커에 대해 다룰 예정인데, 분량이 많아 나눠 쓰게 될 듯 하다.
